<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=canonical href=https://pei-yan-2147.github.io/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29.html><link rel=prev href=%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%28Machine%20Learning%29.html><link rel=next href=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%BC%94%E7%AE%97%E6%B3%95.html><link rel=icon href=logo/logo.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.5.32"><title>神經網路(Neural Network) - PEI</title><link rel=stylesheet href=assets/stylesheets/main.3cba04c6.min.css><link rel=stylesheet href=assets/stylesheets/palette.06af60db.min.css><script src=https://unpkg.com/iframe-worker/shim></script><script src=assets/pymdownx-extras/material-extra-theme-2ek1P7jT.js type=text/javascript></script><script src=assets/pymdownx-extras/material-extra-3rdparty-BUCF4rjN.js type=text/javascript></script><!-- 
<script type="text/javascript">
  window.MathJax = {
      tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],  // 支持 $...$ 和 \(...\) 语法
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
      },
      options: {
          processEscapes: true
      }
  };
</script> --><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><!-- 背景图片 --><div class=image-background></div><!-- 星空和流星效果的Canvas背景 --><canvas id=starCanvas style="z-index: -2; position: fixed; top: 0; left: 0; width: 100%; height: 100%;"></canvas><!-- 粒子效果的Canvas背景，放置在星空和流星前面 --><canvas id=particleCanvas style="z-index: -1; position: fixed; top: 0; left: 0; width: 100%; height: 100%;"></canvas><script>
document.addEventListener("DOMContentLoaded", function() {
  var tocContainer = document.querySelector('.custom-toc-container');
  var contentInner = document.querySelector('.md-content__inner');

  // 设置星空流星的 Canvas 尺寸
  var starCanvas = document.getElementById('starCanvas');
  var starCtx = starCanvas.getContext('2d');
  function resizeStarCanvas() {
    starCanvas.width = window.innerWidth;
    starCanvas.height = window.innerHeight;
  }
  resizeStarCanvas();
  window.addEventListener('resize', resizeStarCanvas);

  // 设置粒子效果的 Canvas 尺寸
  var particleCanvas = document.getElementById('particleCanvas');
  var particleCtx = particleCanvas.getContext('2d');
  function resizeParticleCanvas() {
    particleCanvas.width = window.innerWidth;
    particleCanvas.height = window.innerHeight;
  }
  resizeParticleCanvas();
  window.addEventListener('resize', resizeParticleCanvas);

  // 星星类
  class Star {
    constructor() {
      this.reset();  // 随机初始化星星的位置、大小和透明度
    }

    reset() {
      this.x = Math.random() * starCanvas.width;
      this.y = Math.random() * starCanvas.height;
      this.originalSize = Math.random() * 2 + 1;
      this.size = 0;
      this.alpha = 0;
      this.growthSpeed = 0.02;
      this.shrinkSpeed = 0.01;
      this.life = Math.random() * 200 + 100;
    }

    update() {
      if (this.life > 100) {
        this.size += this.growthSpeed;
        this.alpha += this.growthSpeed;
        if (this.size > this.originalSize) this.size = this.originalSize;
        if (this.alpha > 1) this.alpha = 1;
      } else {
        this.size -= this.shrinkSpeed;
        this.alpha -= this.shrinkSpeed;
        if (this.size < 0) this.size = 0;
        if (this.alpha < 0) this.alpha = 0;
      }

      this.life -= 1;
      if (this.life <= 0) this.reset();
    }

    draw() {
      starCtx.fillStyle = `rgba(242, 187, 255, ${this.alpha})`;
      starCtx.beginPath();
      starCtx.arc(this.x, this.y, this.size, 0, Math.PI * 2);
      starCtx.closePath();
      starCtx.fill();
    }
  }

  // 流星类
  class Meteor {
    constructor(x = null, y = null) {
      this.isCustom = (x !== null && y !== null); // 判斷是否是滑鼠生成的流星
      this.reset(x, y);
    }

    reset(x = null, y = null) {
      if (this.isCustom) {
        this.x = x;
        this.y = y;
        this.size = this.originalSize; // 立即设为最大值
        this.alpha = 1; // 立即设为最大透明度
      } else {
        this.x = Math.random() * starCanvas.width / 2;
        this.y = Math.random() * starCanvas.height;
        this.size = 0;
        this.alpha = 0;
      }
      this.originalSize = Math.random() * 1 + 1;
      this.growthSpeed = 0.03;
      this.speedX = Math.random() * 8 + 4;
      this.speedY = Math.random() * 4 + 2;
    }

    update() {
      this.x += this.speedX;
      this.y += this.speedY;

      if (!this.isCustom) { // 非滑鼠生成的流星才有增大過程
        this.size += this.growthSpeed;
        this.alpha += this.growthSpeed;
        if (this.size > this.originalSize) this.size = this.originalSize;
        if (this.alpha > 1) this.alpha = 1;
        this.alpha -= 0.025;
      } else {
        // 滑鼠生成的流星更快消失
        this.alpha -= 0.025; // 透明度更快減少
      }

      // 如果是滑鼠生成的流星且透明度為0，將其從數組中移除
      if (this.isCustom && this.alpha <= 0) {
        return true; // 返回true表示可以從數組中移除
      }

      if (!this.isCustom && this.isOutOfView()) {
        this.reset(); // 非滑鼠生成的流星需要重新生成
      }
      return false; // 不移除流星
    }

    draw() {
      starCtx.strokeStyle = `rgba(242, 187, 255, ${this.alpha})`;
      starCtx.lineWidth = this.size;
      starCtx.beginPath();
      starCtx.moveTo(this.x, this.y);
      starCtx.lineTo(this.x - this.speedX * 5, this.y - this.speedY * 5);
      starCtx.stroke();

      starCtx.fillStyle = `rgba(242, 187, 255, ${this.alpha})`;
      this.drawStarShape(this.x, this.y, this.size, 5);
    }

    drawStarShape(cx, cy, outerRadius, spikes) {
      let rot = Math.PI / 2 * 3;
      let step = Math.PI / spikes;

      starCtx.beginPath();
      starCtx.moveTo(cx, cy - outerRadius);
      for (let i = 0; i < spikes; i++) {
        let x = cx + Math.cos(rot) * outerRadius;
        let y = cy + Math.sin(rot) * outerRadius;
        starCtx.lineTo(x, y);
        rot += step;
      }
      starCtx.lineTo(cx, cy - outerRadius);
      starCtx.closePath();
      starCtx.fill();
    }

    isOutOfView() {
      return this.x > starCanvas.width || this.y > starCanvas.height || this.alpha <= 0;
    }
  }

  // 粒子类
  class Particle {
    constructor(x, y) {
      this.x = x;
      this.y = y;
      this.size = Math.random() * 3 + 1;
      this.speedX = Math.random() * 3 - 1.5;
      this.speedY = Math.random() * 3 - 1.5;
      this.alpha = 1;
    }

    update() {
      this.x += this.speedX;
      this.y += this.speedY;
      if (this.size > 0.2) this.size -= 0.1;
      this.alpha -= 0.02;  // 粒子逐渐消失
    }

    draw() {
      particleCtx.fillStyle = `rgba(242, 187, 255, ${this.alpha})`;
      particleCtx.beginPath();
      particleCtx.arc(this.x, this.y, this.size, 0, Math.PI * 2);
      particleCtx.closePath();
      particleCtx.fill();
    }
  }

  // 初始化粒子、星星、流星数组
  let starsArray = [];
  let meteorsArray = [];
  let customMeteorsArray = [];
  let particlesArray = [];
  const maxMeteors = 1;

  // 初始化星星
  function initStars() {
    starsArray = [];
    for (let i = 0; i < 150; i++) {
      starsArray.push(new Star());
    }
  }

  // 生成更多流星
  function createMeteor() {
    if (meteorsArray.length < maxMeteors) {
      meteorsArray.push(new Meteor());
    }
  }

  // 生成粒子
  function createParticle(event) {
    const x = event.x;
    const y = event.y;
    for (let i = 0; i < 1; i++) {
      particlesArray.push(new Particle(x, y));
    }
  }

  // 生成滑鼠流星
  function createCustomMeteor(event) {
    const x = event.clientX;
    const y = event.clientY;
    const newMeteor = new Meteor(x, y);
    newMeteor.speedX = Math.random() * 8 + 4;  // 設置滑鼠流星的速度
    newMeteor.speedY = Math.random() * 4 + 2;
    newMeteor.size = newMeteor.originalSize; // 立即設定最大大小
    newMeteor.alpha = 1; // 立即設定最大透明度
    customMeteorsArray.push(newMeteor);
  }

  // 更新并绘制星星、流星和粒子
  function handleParticles() {
    for (let i = 0; i < particlesArray.length; i++) {
      particlesArray[i].update();
      particlesArray[i].draw();
      if (particlesArray[i].alpha <= 0) {
        particlesArray.splice(i, 1);
        i--;
      }
    }
  }

  function handleStarsAndMeteors() {
    for (let i = 0; i < starsArray.length; i++) {
      starsArray[i].update();
      starsArray[i].draw();
    }

    for (let i = 0; i < meteorsArray.length; i++) {
      meteorsArray[i].update();
      meteorsArray[i].draw();
      if (meteorsArray[i].isOutOfView()) {
        meteorsArray.splice(i, 1);
        i--;
      }
    }

    for (let i = 0; i < customMeteorsArray.length; i++) {
      if (customMeteorsArray[i].update()) {
        customMeteorsArray.splice(i, 1); // 移除透明度為0的滑鼠生成流星
        i--;
      } else {
        customMeteorsArray[i].draw();
      }
    }
  }

  // 动画循环
  function animate() {
    starCtx.clearRect(0, 0, starCanvas.width, starCanvas.height);
    particleCtx.clearRect(0, 0, particleCanvas.width, particleCanvas.height);
    handleStarsAndMeteors();
    handleParticles();
    requestAnimationFrame(animate);
  }

  // 初始化并开始动画
  initStars();
  animate();

  // 流星的生成频率增加
  setInterval(createMeteor, 500);

  // 监听鼠标事件生成粒子
  window.addEventListener('mousemove', createParticle);

  // // 监听鼠标点击生成额外流星
  // window.addEventListener('click', createCustomMeteor);


  // 处理图像大小调整的逻辑
  document.querySelectorAll('img').forEach(function(img) {
      const match = img.src.match(/=(\d+)%x$/);
      if (match) {
          const widthPercentage = match[1];
          img.style.width = widthPercentage + '%';
          img.style.height = 'auto';
          img.src = img.src.replace(/=(\d+)%x$/, '');
      }
  });
});
</script><!-- Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-RCGHEYTD16"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-RCGHEYTD16');
</script><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=assets/pymdownx-extras/extra-8611f6c398.css><script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=dracula data-md-color-primary=deep-purple data-md-color-accent=deep-purple> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#神經元 class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> <!-- <a href="https://github.com/pei-yan-2147">Sponsorship</a>

is now available!
<span class="twemoji heart-throb">
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M7.655 14.916v-.001h-.002l-.006-.003-.018-.01a22.066 22.066 0 0 1-3.744-2.584C2.045 10.731 0 8.35 0 5.5 0 2.836 2.086 1 4.25 1 5.797 1 7.153 1.802 8 3.02 8.847 1.802 10.203 1 11.75 1 13.914 1 16 2.836 16 5.5c0 2.85-2.044 5.231-3.886 6.818a22.094 22.094 0 0 1-3.433 2.414 7.152 7.152 0 0 1-.31.17l-.018.01-.008.004a.75.75 0 0 1-.69 0Z"/></svg>
</span> --> <!-- ---------------------------------------------------------------------------------- --> 近期更新<a href=機器學習%28Machine%20Learning%29.html>人工智慧</a> </div> <script>var content,el=document.querySelector("[data-md-component=announce]");el&&(content=el.querySelector(".md-typeset"),__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0))</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=header.title> <a href=https://pei-yan-2147.github.io/ title=PEI class="md-header__button md-logo" aria-label=PEI> <img src=logo/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> PEI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 神經網路(Neural Network) </span> </div> </div> </div> <div class=md-header__options> <div class="md-header-nav__scheme md-header-nav__button md-source__icon md-icon"> <a href=javascript:toggleScheme(); title="Light mode" class=light-mode> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg> </a> <a href=javascript:toggleScheme(); title="Dark mode" class=dark-mode> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg> </a> <a href=javascript:toggleScheme(); title="System preference" class=system-mode> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M7.5 2c-1.79 1.15-3 3.18-3 5.5s1.21 4.35 3.03 5.5C4.46 13 2 10.54 2 7.5A5.5 5.5 0 0 1 7.5 2m11.57 1.5 1.43 1.43L4.93 20.5 3.5 19.07 19.07 3.5m-6.18 2.43L11.41 5 9.97 6l.42-1.7L9 3.24l1.75-.12.58-1.65L12 3.1l1.73.03-1.35 1.13.51 1.67m-3.3 3.61-1.16-.73-1.12.78.34-1.32-1.09-.83 1.36-.09.45-1.29.51 1.27 1.36.03-1.05.87.4 1.31M19 13.5a5.5 5.5 0 0 1-5.5 5.5c-1.22 0-2.35-.4-3.26-1.07l7.69-7.69c.67.91 1.07 2.04 1.07 3.26m-4.4 6.58 2.77-1.15-.24 3.35-2.53-2.2m4.33-2.7 1.15-2.77 2.2 2.54-3.35.23m1.15-4.96-1.14-2.78 3.34.24-2.2 2.54M9.63 18.93l2.77 1.15-2.53 2.19-.24-3.34Z"/></svg> </a> <a href=javascript:toggleScheme(); title="Unknown scheme" class=unknown-mode> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m15.07 11.25-.9.92C13.45 12.89 13 13.5 13 15h-2v-.5c0-1.11.45-2.11 1.17-2.83l1.24-1.26c.37-.36.59-.86.59-1.41a2 2 0 0 0-2-2 2 2 0 0 0-2 2H8a4 4 0 0 1 4-4 4 4 0 0 1 4 4 3.2 3.2 0 0 1-.93 2.25M13 19h-2v-2h2M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10c0-5.53-4.5-10-10-10Z"/></svg> </a> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> 演算法筆記 </a> </li> <li class=md-tabs__item> <a href=mack%E5%BE%AE%E7%A9%8D%E5%88%86.html class=md-tabs__link> mack微積分 </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%28Machine%20Learning%29.html class=md-tabs__link> 人工智慧 </a> </li> <li class=md-tabs__item> <a href=%E5%9F%BA%E7%A4%8E%E5%BE%AE%E7%A9%8D%E5%88%86.html class=md-tabs__link> 基礎微積分 </a> </li> <li class=md-tabs__item> <a href=%E6%BB%BE%E7%90%83%E5%88%86%E9%9A%8A.html class=md-tabs__link> 滾球分隊 </a> </li> <li class=md-tabs__item> <a href=mkdocs%E7%AD%86%E8%A8%98.html class=md-tabs__link> mkdocs筆記 </a> </li> <li class=md-tabs__item> <a href=LaTeX.html class=md-tabs__link> LaTeX </a> </li> <li class=md-tabs__item> <a href=%E6%B8%AC%E8%A9%A6.html class=md-tabs__link> 測試 </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=. title=PEI class="md-nav__button md-logo" aria-label=PEI data-md-component=logo> <img src=logo/logo.png alt=logo> </a> PEI </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> 演算法筆記 </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> 演算法筆記 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_2> <label class=md-nav__link for=__nav_1_2 id=__nav_1_2_label tabindex=0> <span class=md-ellipsis> 競賽準備 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_1_2> <span class="md-nav__icon md-icon"></span> 競賽準備 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=%E5%BE%85%E8%BE%A6%E4%BA%8B%E9%A0%85.html class=md-nav__link> <span class=md-ellipsis> 待辦事項 </span> </a> </li> <li class=md-nav__item> <a href=%E7%B5%95%E5%B0%8D%E8%A6%81%E8%A8%98%E5%BE%97.html class=md-nav__link> <span class=md-ellipsis> 絕對要記得 </span> </a> </li> <li class=md-nav__item> <a href=%E8%80%83%E5%89%8D%E5%BF%85%E7%9C%8B.html class=md-nav__link> <span class=md-ellipsis> 考前必看 </span> </a> </li> <li class=md-nav__item> <a href=%E6%9C%89%E4%BA%8B%E6%B2%92%E4%BA%8B%E6%89%93%E4%B8%80%E6%AC%A1.html class=md-nav__link> <span class=md-ellipsis> 有事沒事打一次 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_3> <label class=md-nav__link for=__nav_1_3 id=__nav_1_3_label tabindex=0> <span class=md-ellipsis> 概念、語法、技巧 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_1_3> <span class="md-nav__icon md-icon"></span> 概念、語法、技巧 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=%E8%A4%87%E9%9B%9C%E5%BA%A6.html class=md-nav__link> <span class=md-ellipsis> 複雜度 </span> </a> </li> <li class=md-nav__item> <a href=%E7%B6%93%E5%85%B8%E5%A0%B1%E9%8C%AF.html class=md-nav__link> <span class=md-ellipsis> 經典報錯 </span> </a> </li> <li class=md-nav__item> <a href=%E5%81%B7%E6%87%B6%E6%96%B9%E6%B3%95.html class=md-nav__link> <span class=md-ellipsis> 偷懶方法 </span> </a> </li> <li class=md-nav__item> <a href=%E6%87%B6%E4%BA%BA%E8%BC%B8%E5%85%A5%E6%B3%95%EF%BC%8C%E6%AF%94%E8%B3%BD%E6%99%82%E7%94%A8.html class=md-nav__link> <span class=md-ellipsis> 懶人輸入法，比賽時用 </span> </a> </li> <li class=md-nav__item> <a href=%E5%B8%B8%E7%94%A8%E5%85%A7%E5%BB%BA%E5%87%BD%E6%95%B8.html class=md-nav__link> <span class=md-ellipsis> 常用內建函數 </span> </a> </li> <li class=md-nav__item> <a href=%E6%8C%87%E6%A8%99.html class=md-nav__link> <span class=md-ellipsis> 指標(*、&) </span> </a> </li> <li class=md-nav__item> <a href=getline%E6%90%AD%E9%85%8Dstringstream.html class=md-nav__link> <span class=md-ellipsis> getline搭配stringstream </span> </a> </li> <li class=md-nav__item> <a href=scanf%20%26%20printf.html class=md-nav__link> <span class=md-ellipsis> scanf & printf </span> </a> </li> <li class=md-nav__item> <a href=%E4%BD%8D%E5%85%83%E9%81%8B%E7%AE%97.html class=md-nav__link> <span class=md-ellipsis> 位元運算 </span> </a> </li> <li class=md-nav__item> <a href=%E4%BD%BF%E7%94%A8%E4%BD%8D%E5%85%83%E7%AA%AE%E8%88%89.html class=md-nav__link> <span class=md-ellipsis> 使用位元窮舉 </span> </a> </li> <li class=md-nav__item> <a href=bitset%E4%BD%BF%E7%94%A8.html class=md-nav__link> <span class=md-ellipsis> bitset使用 </span> </a> </li> <li class=md-nav__item> <a href=%E6%8E%92%E5%BA%8Fsort.html class=md-nav__link> <span class=md-ellipsis> 排序sort </span> </a> </li> <li class=md-nav__item> <a href=%E5%AE%9A%E7%BE%A9%E6%AF%94%E8%BC%83%E8%A6%8F%E5%89%87.html class=md-nav__link> <span class=md-ellipsis> 定義比較規則 </span> </a> </li> <li class=md-nav__item> <a href=%E6%8E%92%E5%88%97%E7%94%9F%E6%88%90permutation%E7%AA%AE%E8%88%89.html class=md-nav__link> <span class=md-ellipsis> 排列生成permutation窮舉 </span> </a> </li> <li class=md-nav__item> <a href=switch%E4%B9%8B%E4%BD%BF%E7%94%A8.html class=md-nav__link> <span class=md-ellipsis> switch之使用 </span> </a> </li> <li class=md-nav__item> <a href=do%20while%E4%BD%BF%E7%94%A8.html class=md-nav__link> <span class=md-ellipsis> do while使用 </span> </a> </li> <li class=md-nav__item> <a href=%E5%88%A4%E6%96%B7%E6%98%AF%E5%90%A6%E7%82%BA%E6%95%B8%E5%AD%97%E3%80%81%E5%AD%97%E6%AF%8D.html class=md-nav__link> <span class=md-ellipsis> 判斷是否為數字、字母 </span> </a> </li> <li class=md-nav__item> <a href=%E5%A4%A7%E5%B0%8F%E5%AF%AB%E8%BD%89%E6%8F%9B.html class=md-nav__link> <span class=md-ellipsis> 大小寫轉換 </span> </a> </li> <li class=md-nav__item> <a href=%E5%B0%8F%E6%95%B8%E9%BB%9E%E5%BE%8C%E5%B9%BE%E4%BD%8D.html class=md-nav__link> <span class=md-ellipsis> 小數點後幾位 </span> </a> </li> <li class=md-nav__item> <a href=%E8%BC%B8%E5%87%BA%E7%B5%90%E5%B0%BE%E7%A9%BA%E7%99%BD.html class=md-nav__link> <span class=md-ellipsis> 輸出結尾空白 </span> </a> </li> <li class=md-nav__item> <a href=%E5%89%8D%E5%BA%8F%E3%80%81%E4%B8%AD%E5%BA%8F%E3%80%81%E5%BE%8C%E5%BA%8F.html class=md-nav__link> <span class=md-ellipsis> 前序、中序、後序 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> 數論 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> 數論 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=log.html class=md-nav__link> <span class=md-ellipsis> log </span> </a> </li> <li class=md-nav__item> <a href=%CE%A3%20sigma%20%28%E8%A5%BF%E6%A0%BC%E7%91%AA%29.html class=md-nav__link> <span class=md-ellipsis> Σ sigma (西格瑪) </span> </a> </li> <li class=md-nav__item> <a href=%E6%A8%A1%E9%81%8B%E7%AE%97.html class=md-nav__link> <span class=md-ellipsis> %模運算 </span> </a> </li> <li class=md-nav__item> <a href=%E8%B2%BB%E9%A6%AC%E5%B0%8F%E5%AE%9A%E7%90%86.html class=md-nav__link> <span class=md-ellipsis> 元素/費馬小定理 </span> </a> </li> <li class=md-nav__item> <a href=%E5%9C%88%E7%B5%90%E6%A7%8B%26%E7%B4%84%E7%91%9F%E5%A4%AB%E5%95%8F%E9%A1%8C.html class=md-nav__link> <span class=md-ellipsis> 圈結構&約瑟夫問題 </span> </a> </li> <li class=md-nav__item> <a href=%E4%B8%83%E6%A9%8B%E5%95%8F%E9%A1%8C%26%E4%B8%80%E7%AD%86%E7%95%AB%E5%95%8F%E9%A1%8C.html class=md-nav__link> <span class=md-ellipsis> 七橋問題&一筆畫問題 </span> </a> </li> <li class=md-nav__item> <a href=%E8%B3%AA%E6%95%B8%E3%80%81%E6%89%BE%E8%B3%AA%E5%9B%A0%E6%95%B8%28%E5%9F%83%E6%B0%8F%E7%AF%A9%29.html class=md-nav__link> <span class=md-ellipsis> 質數、找質因數(埃氏篩) </span> </a> </li> <li class=md-nav__item> <a href=%E5%9B%A0%E6%95%B8%E5%80%8D%E6%95%B8.html class=md-nav__link> <span class=md-ellipsis> 因數倍數 </span> </a> </li> <li class=md-nav__item> <a href=%E6%AD%90%E5%B9%BE%E9%87%8C%E5%BE%97%E5%BC%95%E7%90%86.html class=md-nav__link> <span class=md-ellipsis> 歐幾里得引理 </span> </a> </li> <li class=md-nav__item> <a href=%E6%AD%90%E5%B9%BE%E9%87%8C%E5%BE%97%E6%BC%94%E7%AE%97%E6%B3%95%EF%BC%9B%E8%BC%BE%E8%BD%89%E7%9B%B8%E9%99%A4%E6%B3%95%EF%BC%9B%E6%9C%80%E5%A4%A7%E5%85%AC%E5%9B%A0%E6%95%B8gcd.html class=md-nav__link> <span class=md-ellipsis> 歐幾里得演算法；輾轉相除法；最大公因數gcd </span> </a> </li> <li class=md-nav__item> <a href=%E6%93%B4%E5%B1%95%E6%AD%90%E5%B9%BE%E9%87%8C%E5%BE%97%E6%BC%94%E7%AE%97%E6%B3%95.html class=md-nav__link> <span class=md-ellipsis> 擴展歐幾里得演算法 </span> </a> </li> <li class=md-nav__item> <a href=%E6%AD%90%E6%8B%89%E5%87%BD%E6%95%B8.html class=md-nav__link> <span class=md-ellipsis> 歐拉函數 </span> </a> </li> <li class=md-nav__item> <a href=%E7%9F%A9%E9%99%A3%E9%81%8B%E7%AE%97.html class=md-nav__link> <span class=md-ellipsis> 矩陣運算 </span> </a> </li> <li class=md-nav__item> <a href=%E5%8D%A1%E5%A1%94%E8%98%AD%E6%95%B8.html class=md-nav__link> <span class=md-ellipsis> 卡塔蘭數(Catalan Numbers) </span> </a> </li> <li class=md-nav__item> <a href=%E5%A4%A7%E6%95%B8%E9%81%8B%E7%AE%97.html class=md-nav__link> <span class=md-ellipsis> 大數運算 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_5> <label class=md-nav__link for=__nav_1_5 id=__nav_1_5_label tabindex=0> <span class=md-ellipsis> 資料結構 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_5_label aria-expanded=false> <label class=md-nav__title for=__nav_1_5> <span class="md-nav__icon md-icon"></span> 資料結構 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=STL.html class=md-nav__link> <span class=md-ellipsis> STL </span> </a> </li> <li class=md-nav__item> <a href=%E5%AD%97%E5%85%83%E3%80%81%E5%AD%97%E4%B8%B2.html class=md-nav__link> <span class=md-ellipsis> 字元、字串 </span> </a> </li> <li class=md-nav__item> <a href=%E9%8F%88%E7%B5%90%E7%B5%90%E6%A7%8B.html class=md-nav__link> <span class=md-ellipsis> 鏈結結構 </span> </a> </li> <li class=md-nav__item> <a href=map.html class=md-nav__link> <span class=md-ellipsis> map </span> </a> </li> <li class=md-nav__item> <a href=%E5%84%AA%E5%85%88%E6%9F%B1%E5%88%97%28priority_queue%29.html class=md-nav__link> <span class=md-ellipsis> 優先柱列(priority_queue) </span> </a> </li> <li class=md-nav__item> <a href=%E7%B7%9A%E6%AE%B5%E6%A8%B9.html class=md-nav__link> <span class=md-ellipsis> 線段樹 </span> </a> </li> <li class=md-nav__item> <a href=%E5%AD%97%E5%85%B8%E6%A8%B9%20trie.html class=md-nav__link> <span class=md-ellipsis> 字典樹 trie </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_6> <label class=md-nav__link for=__nav_1_6 id=__nav_1_6_label tabindex=0> <span class=md-ellipsis> 演算法 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_6_label aria-expanded=false> <label class=md-nav__title for=__nav_1_6> <span class="md-nav__icon md-icon"></span> 演算法 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=%E9%81%9E%E8%BF%B4.html class=md-nav__link> <span class=md-ellipsis> 遞迴 </span> </a> </li> <li class=md-nav__item> <a href=%E7%88%86%E6%90%9C.html class=md-nav__link> <span class=md-ellipsis> 爆搜 </span> </a> </li> <li class=md-nav__item> <a href=%E5%89%8D%E7%B6%B4%E5%92%8C%26%E5%B7%AE%E5%88%86.html class=md-nav__link> <span class=md-ellipsis> 前綴和&差分 </span> </a> </li> <li class=md-nav__item> <a href=%E6%90%9C%E5%B0%8B%E3%80%81%E4%BA%8C%E5%88%86%E6%90%9C%E5%B0%8B%E3%80%81%E8%B7%B3%E8%BA%8D%E4%BA%8C%E5%88%86%E6%90%9C.html class=md-nav__link> <span class=md-ellipsis> 搜尋、二分搜尋、跳躍二分搜 </span> </a> </li> <li class=md-nav__item> <a href=%E5%93%88%E5%B8%8C%E8%A1%A8.html class=md-nav__link> <span class=md-ellipsis> 哈希表 </span> </a> </li> <li class=md-nav__item> <a href=%E9%9B%A2%E6%95%A3%E5%8C%96.html class=md-nav__link> <span class=md-ellipsis> 離散化 </span> </a> </li> <li class=md-nav__item> <a href=%E5%BF%AB%E9%80%9F%E5%86%AA.html class=md-nav__link> <span class=md-ellipsis> 快速冪 </span> </a> </li> <li class=md-nav__item> <a href=%E6%BB%91%E5%8B%95%E8%A6%96%E7%AA%97%28Sliding%20window%29%28%E6%8A%80%E5%B7%A7%29.html class=md-nav__link> <span class=md-ellipsis> 滑動視窗(Sliding window)(技巧) </span> </a> </li> <li class=md-nav__item> <a href=%E6%8E%83%E7%9E%84%E7%B7%9A%E6%BC%94%E7%AE%97%E6%B3%95.html class=md-nav__link> <span class=md-ellipsis> 掃瞄線演算法 </span> </a> </li> <li class=md-nav__item> <a href=/%E7%88%86%E6%90%9C.html#%E4%B8%89%E6%8A%98%E5%8D%8A%E6%9E%9A%E8%88%89 class=md-nav__link> <span class=md-ellipsis> 折半枚舉 </span> </a> </li> <li class=md-nav__item> <a href=%E8%B2%AA%E5%BF%83%E6%BC%94%E7%AE%97%E6%B3%95%28Greedy%29.html class=md-nav__link> <span class=md-ellipsis> 貪心演算法(Greedy) </span> </a> </li> <li class=md-nav__item> <a href=%E5%8B%95%E6%85%8B%E8%A6%8F%E5%8A%83%28dp%29.html class=md-nav__link> <span class=md-ellipsis> 動態規劃(dp) </span> </a> </li> <li class=md-nav__item> <a href=%E5%9C%96%E8%AB%96.html class=md-nav__link> <span class=md-ellipsis> 圖論 </span> </a> </li> <li class=md-nav__item> <a href=%E6%A8%B9%E8%AB%96.html class=md-nav__link> <span class=md-ellipsis> 樹論 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_7> <label class=md-nav__link for=__nav_1_7 id=__nav_1_7_label tabindex=0> <span class=md-ellipsis> 刷題 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_7_label aria-expanded=false> <label class=md-nav__title for=__nav_1_7> <span class="md-nav__icon md-icon"></span> 刷題 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_7_1> <label class=md-nav__link for=__nav_1_7_1 id=__nav_1_7_1_label tabindex=0> <span class=md-ellipsis> CSEC </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_1_7_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1_7_1> <span class="md-nav__icon md-icon"></span> CSEC </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=CSEC.html class=md-nav__link> <span class=md-ellipsis> 首頁 </span> </a> </li> <li class=md-nav__item> <a href=CSES%20Problem%20Set.html class=md-nav__link> <span class=md-ellipsis> CSES Problem Set </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=%E5%8C%97%E5%B8%82%E5%AD%B8%E7%A7%91%E8%83%BD%E5%8A%9B%E7%AB%B6%E8%B3%BD.html class=md-nav__link> <span class=md-ellipsis> 北市學科能力競賽 </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_7_3> <label class=md-nav__link for=__nav_1_7_3 id=__nav_1_7_3_label tabindex=0> <span class=md-ellipsis> APCS歷屆 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_1_7_3_label aria-expanded=false> <label class=md-nav__title for=__nav_1_7_3> <span class="md-nav__icon md-icon"></span> APCS歷屆 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=%E7%B7%A8%E5%B9%B4%E9%A1%8C%E5%BA%AB%28by%20YUI%20HUANG%29.html class=md-nav__link> <span class=md-ellipsis> 編年/題庫(by YUI HUANG) </span> </a> </li> <li class=md-nav__item> <a href=%E5%AF%A6%E4%BD%9C%E4%B8%80.html class=md-nav__link> <span class=md-ellipsis> 實作一 </span> </a> </li> <li class=md-nav__item> <a href=%E5%AF%A6%E4%BD%9C%E4%BA%8C.html class=md-nav__link> <span class=md-ellipsis> 實作二 </span> </a> </li> <li class=md-nav__item> <a href=%E5%AF%A6%E4%BD%9C%E4%B8%89.html class=md-nav__link> <span class=md-ellipsis> 實作三 </span> </a> </li> <li class=md-nav__item> <a href=%E5%AF%A6%E4%BD%9C%E5%9B%9B.html class=md-nav__link> <span class=md-ellipsis> 實作四 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_8> <label class=md-nav__link for=__nav_1_8 id=__nav_1_8_label tabindex=0> <span class=md-ellipsis> 其他 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_8_label aria-expanded=false> <label class=md-nav__title for=__nav_1_8> <span class="md-nav__icon md-icon"></span> 其他 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=APCS.html class=md-nav__link> <span class=md-ellipsis> APCS </span> </a> </li> <li class=md-nav__item> <a href=%E4%BD%BF%E7%94%A8vscode%E6%92%B0%E5%AF%ABc%2B%2B.html class=md-nav__link> <span class=md-ellipsis> 使用vscode撰寫c++ </span> </a> </li> <li class=md-nav__item> <a href=hackmd%E7%89%B9%E6%AE%8A%E4%BD%BF%E7%94%A8.html class=md-nav__link> <span class=md-ellipsis> hackmd特殊使用 </span> </a> </li> <li class=md-nav__item> <a href=%E5%84%AA%E8%B3%AA%E7%B6%B2%E7%AB%99.html class=md-nav__link> <span class=md-ellipsis> 優質網站 </span> </a> </li> <li class=md-nav__item> <a href=%E7%87%9F%E9%9A%8A.html class=md-nav__link> <span class=md-ellipsis> 營隊 </span> </a> </li> <li class=md-nav__item> <a href=%E6%89%93%E5%AD%97%E9%9F%B3%E6%95%88.html class=md-nav__link> <span class=md-ellipsis> 打字音效 </span> </a> </li> <li class=md-nav__item> <a href=%E6%A1%8C%E9%9D%A2%E8%B2%93%E5%92%AA.html class=md-nav__link> <span class=md-ellipsis> 桌面貓咪 </span> </a> </li> <li class=md-nav__item> <a href=%E8%B3%87%E6%96%99%E4%BE%86%E6%BA%90.html class=md-nav__link> <span class=md-ellipsis> 資料來源 </span> </a> </li> <li class=md-nav__item> <a href=%E5%8A%A0%E6%B7%B1%E5%8A%A0%E5%BB%A3.html class=md-nav__link> <span class=md-ellipsis> 加深加廣 </span> </a> </li> <li class=md-nav__item> <a href=%E5%AD%B8%E7%BF%92%E8%B3%87%E6%BA%90.html class=md-nav__link> <span class=md-ellipsis> 學習資源 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=mack%E5%BE%AE%E7%A9%8D%E5%88%86.html class=md-nav__link> <span class=md-ellipsis> mack微積分 </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex> <span class=md-ellipsis> 人工智慧 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> 人工智慧 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%28Machine%20Learning%29.html class=md-nav__link> <span class=md-ellipsis> 機器學習(Machine Learning) </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 神經網路(Neural Network) </span> <span class="md-nav__icon md-icon"></span> </label> <a href=%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29.html class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 神經網路(Neural Network) </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#神經元 class=md-nav__link> <span class=md-ellipsis> 神經元 </span> </a> <nav class=md-nav aria-label=神經元> <ul class=md-nav__list> <li class=md-nav__item> <a href=#神經元是啥 class=md-nav__link> <span class=md-ellipsis> 神經元是啥 </span> </a> </li> <li class=md-nav__item> <a href=#線性神經元 class=md-nav__link> <span class=md-ellipsis> 線性神經元 </span> </a> </li> <li class=md-nav__item> <a href=#非線性神經元 class=md-nav__link> <span class=md-ellipsis> 非線性神經元 </span> </a> </li> <li class=md-nav__item> <a href=#激活函數activation-function class=md-nav__link> <span class=md-ellipsis> 激活函數(Activation function) </span> </a> <nav class=md-nav aria-label="激活函數(Activation function)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#sign class=md-nav__link> <span class=md-ellipsis> Sign </span> </a> </li> <li class=md-nav__item> <a href=#sigmoid class=md-nav__link> <span class=md-ellipsis> Sigmoid </span> </a> </li> <li class=md-nav__item> <a href=#relu class=md-nav__link> <span class=md-ellipsis> Relu </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#多層神經元 class=md-nav__link> <span class=md-ellipsis> 多層神經元 </span> </a> </li> <li class=md-nav__item> <a href=#多層神經元的計算 class=md-nav__link> <span class=md-ellipsis> 多層神經元的計算 </span> </a> <nav class=md-nav aria-label=多層神經元的計算> <ul class=md-nav__list> <li class=md-nav__item> <a href=#回歸問題regression-problem class=md-nav__link> <span class=md-ellipsis> 回歸問題(Regression Problem) </span> </a> </li> <li class=md-nav__item> <a href=#分類問題classification-problem class=md-nav__link> <span class=md-ellipsis> 分類問題(Classification Problem) </span> </a> <nav class=md-nav aria-label="分類問題(Classification Problem)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#softmax函數 class=md-nav__link> <span class=md-ellipsis> Softmax函數 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#損失函數 class=md-nav__link> <span class=md-ellipsis> 損失函數 </span> </a> <nav class=md-nav aria-label=損失函數> <ul class=md-nav__list> <li class=md-nav__item> <a href=#回歸問題的損失函數 class=md-nav__link> <span class=md-ellipsis> 回歸問題的損失函數 </span> </a> <nav class=md-nav aria-label=回歸問題的損失函數> <ul class=md-nav__list> <li class=md-nav__item> <a href=#l1l2-loss class=md-nav__link> <span class=md-ellipsis> L1/L2 loss </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#分類問題的損失函數 class=md-nav__link> <span class=md-ellipsis> 分類問題的損失函數 </span> </a> <nav class=md-nav aria-label=分類問題的損失函數> <ul class=md-nav__list> <li class=md-nav__item> <a href=#01-loss class=md-nav__link> <span class=md-ellipsis> 0/1 loss </span> </a> </li> <li class=md-nav__item> <a href=#negative-log-likelihoodnll class=md-nav__link> <span class=md-ellipsis> Negative Log-Likelihood(NLL) </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#梯度下降演算法 class=md-nav__link> <span class=md-ellipsis> 梯度下降演算法 </span> </a> <nav class=md-nav aria-label=梯度下降演算法> <ul class=md-nav__list> <li class=md-nav__item> <a href=#一元函數梯度下降演算法 class=md-nav__link> <span class=md-ellipsis> 一元函數梯度下降演算法 </span> </a> <nav class=md-nav aria-label=一元函數梯度下降演算法> <ul class=md-nav__list> <li class=md-nav__item> <a href=#數學式 class=md-nav__link> <span class=md-ellipsis> 數學式 </span> </a> </li> <li class=md-nav__item> <a href=#模擬 class=md-nav__link> <span class=md-ellipsis> 模擬 </span> </a> </li> <li class=md-nav__item> <a href=#備註 class=md-nav__link> <span class=md-ellipsis> 備註 </span> </a> <nav class=md-nav aria-label=備註> <ul class=md-nav__list> <li class=md-nav__item> <a href=#一元函數-univariate-function class=md-nav__link> <span class=md-ellipsis> 一元函數 (Univariate Function) </span> </a> </li> <li class=md-nav__item> <a href=#多元函數-multivariate-function class=md-nav__link> <span class=md-ellipsis> 多元函數 (Multivariate Function) </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#多元函數梯度下降演算法 class=md-nav__link> <span class=md-ellipsis> 多元函數梯度下降演算法 </span> </a> <nav class=md-nav aria-label=多元函數梯度下降演算法> <ul class=md-nav__list> <li class=md-nav__item> <a href=#計算流程 class=md-nav__link> <span class=md-ellipsis> 計算流程 </span> </a> </li> <li class=md-nav__item> <a href=#模擬_1 class=md-nav__link> <span class=md-ellipsis> 模擬 </span> </a> </li> <li class=md-nav__item> <a href=#補充 class=md-nav__link> <span class=md-ellipsis> 補充 </span> </a> </li> <li class=md-nav__item> <a href=#備註_1 class=md-nav__link> <span class=md-ellipsis> 備註 </span> </a> <nav class=md-nav aria-label=備註> <ul class=md-nav__list> <li class=md-nav__item> <a href=#偏微分 class=md-nav__link> <span class=md-ellipsis> 偏微分 </span> </a> </li> <li class=md-nav__item> <a href=#batch class=md-nav__link> <span class=md-ellipsis> Batch </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#幫助訓練神經網路 class=md-nav__link> <span class=md-ellipsis> 幫助訓練神經網路 </span> </a> <nav class=md-nav aria-label=幫助訓練神經網路> <ul class=md-nav__list> <li class=md-nav__item> <a href=#dropout class=md-nav__link> <span class=md-ellipsis> Dropout </span> </a> </li> <li class=md-nav__item> <a href=#batch-normalization class=md-nav__link> <span class=md-ellipsis> Batch Normalization </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%BC%94%E7%AE%97%E6%B3%95.html class=md-nav__link> <span class=md-ellipsis> 梯度下降演算法 </span> </a> </li> <li class=md-nav__item> <a href=%E9%80%B2%E5%BA%A6.html class=md-nav__link> <span class=md-ellipsis> 進度 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=%E5%9F%BA%E7%A4%8E%E5%BE%AE%E7%A9%8D%E5%88%86.html class=md-nav__link> <span class=md-ellipsis> 基礎微積分 </span> </a> </li> <li class=md-nav__item> <a href=%E6%BB%BE%E7%90%83%E5%88%86%E9%9A%8A.html class=md-nav__link> <span class=md-ellipsis> 滾球分隊 </span> </a> </li> <li class=md-nav__item> <a href=mkdocs%E7%AD%86%E8%A8%98.html class=md-nav__link> <span class=md-ellipsis> mkdocs筆記 </span> </a> </li> <li class=md-nav__item> <a href=LaTeX.html class=md-nav__link> <span class=md-ellipsis> LaTeX </span> </a> </li> <li class=md-nav__item> <a href=%E6%B8%AC%E8%A9%A6.html class=md-nav__link> <span class=md-ellipsis> 測試 </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#神經元 class=md-nav__link> <span class=md-ellipsis> 神經元 </span> </a> <nav class=md-nav aria-label=神經元> <ul class=md-nav__list> <li class=md-nav__item> <a href=#神經元是啥 class=md-nav__link> <span class=md-ellipsis> 神經元是啥 </span> </a> </li> <li class=md-nav__item> <a href=#線性神經元 class=md-nav__link> <span class=md-ellipsis> 線性神經元 </span> </a> </li> <li class=md-nav__item> <a href=#非線性神經元 class=md-nav__link> <span class=md-ellipsis> 非線性神經元 </span> </a> </li> <li class=md-nav__item> <a href=#激活函數activation-function class=md-nav__link> <span class=md-ellipsis> 激活函數(Activation function) </span> </a> <nav class=md-nav aria-label="激活函數(Activation function)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#sign class=md-nav__link> <span class=md-ellipsis> Sign </span> </a> </li> <li class=md-nav__item> <a href=#sigmoid class=md-nav__link> <span class=md-ellipsis> Sigmoid </span> </a> </li> <li class=md-nav__item> <a href=#relu class=md-nav__link> <span class=md-ellipsis> Relu </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#多層神經元 class=md-nav__link> <span class=md-ellipsis> 多層神經元 </span> </a> </li> <li class=md-nav__item> <a href=#多層神經元的計算 class=md-nav__link> <span class=md-ellipsis> 多層神經元的計算 </span> </a> <nav class=md-nav aria-label=多層神經元的計算> <ul class=md-nav__list> <li class=md-nav__item> <a href=#回歸問題regression-problem class=md-nav__link> <span class=md-ellipsis> 回歸問題(Regression Problem) </span> </a> </li> <li class=md-nav__item> <a href=#分類問題classification-problem class=md-nav__link> <span class=md-ellipsis> 分類問題(Classification Problem) </span> </a> <nav class=md-nav aria-label="分類問題(Classification Problem)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#softmax函數 class=md-nav__link> <span class=md-ellipsis> Softmax函數 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#損失函數 class=md-nav__link> <span class=md-ellipsis> 損失函數 </span> </a> <nav class=md-nav aria-label=損失函數> <ul class=md-nav__list> <li class=md-nav__item> <a href=#回歸問題的損失函數 class=md-nav__link> <span class=md-ellipsis> 回歸問題的損失函數 </span> </a> <nav class=md-nav aria-label=回歸問題的損失函數> <ul class=md-nav__list> <li class=md-nav__item> <a href=#l1l2-loss class=md-nav__link> <span class=md-ellipsis> L1/L2 loss </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#分類問題的損失函數 class=md-nav__link> <span class=md-ellipsis> 分類問題的損失函數 </span> </a> <nav class=md-nav aria-label=分類問題的損失函數> <ul class=md-nav__list> <li class=md-nav__item> <a href=#01-loss class=md-nav__link> <span class=md-ellipsis> 0/1 loss </span> </a> </li> <li class=md-nav__item> <a href=#negative-log-likelihoodnll class=md-nav__link> <span class=md-ellipsis> Negative Log-Likelihood(NLL) </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#梯度下降演算法 class=md-nav__link> <span class=md-ellipsis> 梯度下降演算法 </span> </a> <nav class=md-nav aria-label=梯度下降演算法> <ul class=md-nav__list> <li class=md-nav__item> <a href=#一元函數梯度下降演算法 class=md-nav__link> <span class=md-ellipsis> 一元函數梯度下降演算法 </span> </a> <nav class=md-nav aria-label=一元函數梯度下降演算法> <ul class=md-nav__list> <li class=md-nav__item> <a href=#數學式 class=md-nav__link> <span class=md-ellipsis> 數學式 </span> </a> </li> <li class=md-nav__item> <a href=#模擬 class=md-nav__link> <span class=md-ellipsis> 模擬 </span> </a> </li> <li class=md-nav__item> <a href=#備註 class=md-nav__link> <span class=md-ellipsis> 備註 </span> </a> <nav class=md-nav aria-label=備註> <ul class=md-nav__list> <li class=md-nav__item> <a href=#一元函數-univariate-function class=md-nav__link> <span class=md-ellipsis> 一元函數 (Univariate Function) </span> </a> </li> <li class=md-nav__item> <a href=#多元函數-multivariate-function class=md-nav__link> <span class=md-ellipsis> 多元函數 (Multivariate Function) </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#多元函數梯度下降演算法 class=md-nav__link> <span class=md-ellipsis> 多元函數梯度下降演算法 </span> </a> <nav class=md-nav aria-label=多元函數梯度下降演算法> <ul class=md-nav__list> <li class=md-nav__item> <a href=#計算流程 class=md-nav__link> <span class=md-ellipsis> 計算流程 </span> </a> </li> <li class=md-nav__item> <a href=#模擬_1 class=md-nav__link> <span class=md-ellipsis> 模擬 </span> </a> </li> <li class=md-nav__item> <a href=#補充 class=md-nav__link> <span class=md-ellipsis> 補充 </span> </a> </li> <li class=md-nav__item> <a href=#備註_1 class=md-nav__link> <span class=md-ellipsis> 備註 </span> </a> <nav class=md-nav aria-label=備註> <ul class=md-nav__list> <li class=md-nav__item> <a href=#偏微分 class=md-nav__link> <span class=md-ellipsis> 偏微分 </span> </a> </li> <li class=md-nav__item> <a href=#batch class=md-nav__link> <span class=md-ellipsis> Batch </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#幫助訓練神經網路 class=md-nav__link> <span class=md-ellipsis> 幫助訓練神經網路 </span> </a> <nav class=md-nav aria-label=幫助訓練神經網路> <ul class=md-nav__list> <li class=md-nav__item> <a href=#dropout class=md-nav__link> <span class=md-ellipsis> Dropout </span> </a> </li> <li class=md-nav__item> <a href=#batch-normalization class=md-nav__link> <span class=md-ellipsis> Batch Normalization </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1>神經網路(Neural Network)</h1> <h2 id=神經元>神經元<a class=headerlink href=#神經元 title="Permanent link"></a></h2> <h3 id=神經元是啥>神經元是啥<a class=headerlink href=#神經元是啥 title="Permanent link"></a></h3> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-1.png><br> (圖片來源：<a href=https://simple.wikipedia.org/wiki/Neuron>維基百科</a>)</p> <ul> <li>Dendrites(樹突)：輸入資料</li> <li>cell body(細胞體)：運算</li> <li>Synapse(突觸)：輸出資料</li> </ul> <h3 id=線性神經元>線性神經元<a class=headerlink href=#線性神經元 title="Permanent link"></a></h3> <p><a href=%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%28Machine%20Learning%29.html#線性模型>線性模型</a>其實就是神經元的應用，有兩個輸入，分別乘上兩個w後再經過激活函數，最後得到<span class=arithmatex>\(f(x)\)</span></p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-3.png></p> <p>若以<a href=%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%28Machine%20Learning%29.html#線性模型>線性模型</a>為例</p> <pre class=arithmatex>\[
f(x) = w_1x_1 + w_2x_2 = w^T x
\]</pre> <pre class=arithmatex>\[
w^T = \begin{bmatrix} 3 \\ -2 \end{bmatrix}, \quad
x = \begin{bmatrix} 6 &amp; 5 \end{bmatrix}
\]</pre> <pre class=arithmatex>\[
w^T \cdot x = \begin{bmatrix} 3 \\ -2 \end{bmatrix} 
\cdot 
\begin{bmatrix} 6 &amp; 5 \end{bmatrix} 
= 3 \cdot 6 + (-2) \cdot 5 = 18 - 10 = 8
\]</pre> <p>我們輸入了兩個數值，<span class=arithmatex>\(x_1\)</span>、<span class=arithmatex>\(x_2\)</span>，最後經過神經元運算得到結果 <span class=arithmatex>\(f(x)=8\)</span>。</p> <h3 id=非線性神經元>非線性神經元<a class=headerlink href=#非線性神經元 title="Permanent link"></a></h3> <p>如果我們要讓神經元變成非線性，我們要加上<a href=#激活函數activation-function>激活函數</a>，如圖。</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-4.png></p> <p>我們先假設<a href=#激活函數activation-function>激活函數</a>為 <span class=arithmatex>\(\sigma(x)\)</span>：</p> <pre class=arithmatex>\[
\sigma(x) =
\begin{cases} 
1, &amp; \text{if } x &gt; 0, \\ 
0, &amp; \text{if } x = 0, \\
-1, &amp; \text{if } x &lt; 0.
\end{cases}
\]</pre> <p>我們將原本的線性函數 <span class=arithmatex>\(f(x)\)</span> 加上<a href=#激活函數activation-function>激活函數</a> <span class=arithmatex>\(g(x)\)</span> ：</p> <pre class=arithmatex>\[
f(x) =\sigma ( w_1x_1 + w_2x_2) = \sigma(w^T x)
\]</pre> <pre class=arithmatex>\[
w^T = \begin{bmatrix} 3 \\ -2 \end{bmatrix}, \quad
x = \begin{bmatrix} 6 &amp; 5 \end{bmatrix}
\]</pre> <pre class=arithmatex>\[
w^T \cdot x = \begin{bmatrix} 3 \\ -2 \end{bmatrix} 
\cdot 
\begin{bmatrix} 6 &amp; 5 \end{bmatrix} 
= 3 \cdot 6 + (-2) \cdot 5 = 18 - 10 = 8
\]</pre> <pre class=arithmatex>\[
f(x)=\sigma(8)=1
\]</pre> <p>所以最後得到1，因為只要結果大於0就是1，所以是非線性。</p> <p>最後我們再看到下圖，可以發現經過激活函數之後，藍色區域的點座標作為輸入時輸出為1(輸出大於0)，紅色區域的點座標作為輸入時輸出為0(輸出小於0)。</p> <blockquote> <p>也就是說，一個神經元可以劃出一條線，做一個二元分類。</p> </blockquote> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-3-1.png></p> <h3 id=激活函數activation-function>激活函數(Activation function)<a class=headerlink href=#激活函數activation-function title="Permanent link"></a></h3> <p>我們要將線性函數轉換為非線性函數時，會使用到激活函數。<br> 以下介紹幾種常見激活函數：</p> <ul> <li><a href=#sign>Sign</a></li> <li><a href=#sigmoid>Sigmoid</a></li> <li><a href=#relu>Relu</a></li> </ul> <h4 id=sign>Sign<a class=headerlink href=#sign title="Permanent link"></a></h4> <p>英文字 "Sign" 的意思是「符號」或「記號」，在數學中特指數值的「正」或「負」。<br> Sign 函數的核心功能是區分數值的符號屬性，並根據數值的符號返回對應的輸出。</p> <p>輸出值為 0 或 1</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/images.png width=50%><br> <a href=https://www.statisticshowto.com/sign-function/ >圖片來源</a></p> <h4 id=sigmoid>Sigmoid<a class=headerlink href=#sigmoid title="Permanent link"></a></h4> <p>"Sigmoid" 來自拉丁語 "sigmoides"，意思是「類似於 S 的形狀」。</p> <p>輸出只有 0 ~ 1 之間，通常使用於機率。</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/download.png width=50%><br> <a href=https://en.wikipedia.org/wiki/Sigmoid_function>圖片來源</a></p> <h4 id=relu>Relu<a class=headerlink href=#relu title="Permanent link"></a></h4> <p>ReLU 是 Rectified Linear Unit 的縮寫，由三部分組成：</p> <ul> <li>Rectified：表示「校正的」，指 ReLU 函數將負值校正為零。</li> <li>Linear：表示當輸入為正值時，輸出的關係是線性的（即輸入值直接作為輸出值）。</li> <li>Unit：源自神經網路的術語，指代「神經元單元」。</li> </ul> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/download-1.png width=50%><br> <a href=https://www.researchgate.net/figure/Graphic-representation-of-the-ReLU-activation-function_fig3_348703101>圖片來源</a></p> <h2 id=多層神經元>多層神經元<a class=headerlink href=#多層神經元 title="Permanent link"></a></h2> <p>剛剛提到，神經元可以劃出一條線進進行二元分類。<br> 那我們看到下圖，這種情況該怎麼辦？<br> 我們希望畫出一條線分開藍點與紅點，但是我們似乎無法用一條線做到，不管是紅線和綠線都沒辦法一次分開。</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-6.png></p> <p>但是如果我們畫出兩條線，看看點是不是在兩條線之間(紅色與綠色重疊區域)就可以判斷出藍色點，如下圖。<br> 也就是說符合「<span class=arithmatex>\(3x-y&gt;0\)</span>、<span class=arithmatex>\(-3x+2y+3&gt;0\)</span>」就算是藍色點。</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-16.png></p> <p>這樣子我們畫出了兩條線，也就是說我們使用了不只一個神經元。<br> 我們以 <span class=arithmatex>\(x=(4,8)\)</span> 為例，下圖為三個神經元及計算過程：<br> 最後計算出來是 1，就代表點被歸類為藍色。</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-17.png></p> <p>我們寫成數學式：</p> <pre class=arithmatex>\[
w_1^T = \begin{bmatrix} 3 \\ -1 \end{bmatrix} , \quad
w_2^T = \begin{bmatrix} -3 \\ 2 \end{bmatrix} , \quad
x = \begin{bmatrix} 4 &amp; 8 \end{bmatrix} 
\]</pre> <pre class=arithmatex>\[
f_1(x) =  w_1^T x , \quad
f_2(x) =  w_2^T x +3
\]</pre> <pre class=arithmatex>\[
g(x) =  sign(f_1(x)+f_2(x)) = 1
\]</pre> <p>這樣我們就成功使用多層神經元做比較困難的事。<br> 從這邊我們就可以理解，遇到要畫圓圈時，我們只要用很多神經元，畫出超多條線就好。</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-18.png></p> <p>而我們最後我們決定好的神經結構，就是<a href=%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%28Machine%20Learning%29.html#假設集合-hypothesis-set>假設集合</a>，如下圖，而<a href=%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%28Machine%20Learning%29.html#學習演算法>學習演算法</a>要挑選出最適合的參數(<span class=arithmatex>\(w\)</span>)。</p> <h2 id=多層神經元的計算>多層神經元的計算<a class=headerlink href=#多層神經元的計算 title="Permanent link"></a></h2> <p>接下來我們要來理解如何計算多層神經元。<br> 從前面的二維神經元例子，我們可以整理出：</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-24.png><br> 圖一：前面的二維神經元例子</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-37.png><br> 圖二：前面的二維神經元例子的整理</p> <p>分別說明</p> <div class="tabbed-set tabbed-alternate" data-tabs=1:5><input checked=checked id=多層神經元的計算-x_ik name=__tabbed_1 type=radio><input id=多層神經元的計算-l_i name=__tabbed_1 type=radio><input id=多層神經元的計算-σ_ik name=__tabbed_1 type=radio><input id=多層神經元的計算-w_ik name=__tabbed_1 type=radio><input id=多層神經元的計算-b_ik name=__tabbed_1 type=radio><div class=tabbed-labels><label for=多層神經元的計算-x_ik><span class=arithmatex>\(x_{(i,k)}\)</span></label><label for=多層神經元的計算-l_i><span class=arithmatex>\(L_{i}\)</span></label><label for=多層神經元的計算-σ_ik><span class=arithmatex>\(σ_{(i,k)}\)</span></label><label for=多層神經元的計算-w_ik><span class=arithmatex>\(w_{(i,k)}\)</span></label><label for=多層神經元的計算-b_ik><span class=arithmatex>\(b_{(i,k)}\)</span></label></div> <div class=tabbed-content> <div class=tabbed-block> <p>第 <span class=arithmatex>\(i\)</span> 層第 <span class=arithmatex>\(k\)</span> 個神經元的輸出</p> </div> <div class=tabbed-block> <p>第 <span class=arithmatex>\(i\)</span> 層(Layer)的全部輸出<br> 也就是所有的 <span class=arithmatex>\(x_{(i,k)}\)</span> 組成的矩陣</p> </div> <div class=tabbed-block> <p>第 <span class=arithmatex>\(i\)</span> 層的第 <span class=arithmatex>\(k\)</span> 個神經元所使用的<a href=#激活函數activation-function>激活函數(Activation function)</a></p> </div> <div class=tabbed-block> <p>第 <span class=arithmatex>\(i\)</span> 層的第 <span class=arithmatex>\(k\)</span> 個神經元的各個權重所組成的矩陣 <span class=arithmatex>\(w\)</span>。<br> 以範例的「<span class=arithmatex>\(-3+2+3=0\)</span>」的紅色線條為例：<span class=arithmatex>\(w = \begin{bmatrix} -3 &amp; 2 \end{bmatrix}\)</span>。</p> </div> <div class=tabbed-block> <p>第 <span class=arithmatex>\(i\)</span> 層的第 <span class=arithmatex>\(k\)</span> 個神經元的偏移量。<br> 以範例的「<span class=arithmatex>\(-3+2+3=0\)</span>」的紅色線條為例，<span class=arithmatex>\(b = 3\)</span>。</p> </div> </div> </div> <p>我們以二維作為例子，但是實際上不一定是二維，可能很多維，假設有 <span class=arithmatex>\(n\)</span> 維，如圖。</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-38.png></p> <p>公式：</p> <pre class=arithmatex>\[
L_{i+1} = 
\begin{bmatrix}
σ_{(i+1,1)}\left(W_{(i+1,1)} \cdot L_i + b_{(i+1,1)}\right), \cdots, 
σ_{(i+1,n)}\left(W_{(i+1,n)} \cdot L_i + b_{(i+1,n)}\right)
\end{bmatrix}
\]</pre> <p>當然，每一層的數量也不一定一樣，如圖：</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-28.png></p> <h3 id=回歸問題regression-problem>回歸問題(Regression Problem)<a class=headerlink href=#回歸問題regression-problem title="Permanent link"></a></h3> <p>在多層神經元要處裡回歸問題時，輸出的會是實數，代表可能是房價或是某數字。<br> 這時我們的最後一層只需要一個神經元，而這個神經元不需要激活函數，直接輸出一個數字即可，如圖， <span class=arithmatex>\(L_m\)</span> 為 <span class=arithmatex>\(1 \times m\)</span> 的矩陣。</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-39.png></p> <h3 id=分類問題classification-problem>分類問題(Classification Problem)<a class=headerlink href=#分類問題classification-problem title="Permanent link"></a></h3> <p>在分類問題時，會使用<a href=#softmax函數> <span class=arithmatex>\(Softmax\)</span> 函數</a>將最後一層的神經元輸出矩陣轉換為機率分布(總和為1)，如圖：</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-40.png></p> <p>所以如果最後一層為 <span class=arithmatex>\(L_m=[1,2,5,3,4]\)</span> ，輸出如圖：<br> 可以看到，假設輸入了一張圖片，如果第三個神經元代表貓，結果顯示「該圖片為貓的機率最高」。</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-35.png></p> <h4 id=softmax函數>Softmax函數<a class=headerlink href=#softmax函數 title="Permanent link"></a></h4> <pre class=arithmatex>\[
Softma𝑥(\mathbf{x_i}) = \frac{e^{x_i}}{\sum^n_j e^{x_j}} = \frac{e^{-m}}{e^{-m}} \frac{e^{x_i}}{\sum^n_j e^{x_j}} = \frac{e^{x_i - m}}{\sum^n_j e^{x_j - m}}
\]</pre> <div class="tabbed-set tabbed-alternate" data-tabs=2:5><input checked=checked id=softmax函數-x name=__tabbed_2 type=radio><input id=softmax函數-e name=__tabbed_2 type=radio><input id=softmax函數-x_i name=__tabbed_2 type=radio><input id=softmax函數-m name=__tabbed_2 type=radio><input id=softmax函數-n name=__tabbed_2 type=radio><div class=tabbed-labels><label for=softmax函數-x>x</label><label for=softmax函數-e><span class=arithmatex>\(e\)</span></label><label for=softmax函數-x_i><span class=arithmatex>\(x_i\)</span></label><label for=softmax函數-m>m</label><label for=softmax函數-n>n</label></div> <div class=tabbed-content> <div class=tabbed-block> <p>所有最後一層的神經元輸出組成的矩陣，如：<span class=arithmatex>\(x=[1,2,5,3,4]\)</span></p> </div> <div class=tabbed-block> <p>自然常數（Euler's number）</p> </div> <div class=tabbed-block> <p>是 <span class=arithmatex>\(x\)</span> 矩陣中的第 <span class=arithmatex>\(i\)</span> 項。</p> </div> <div class=tabbed-block> <p><span class=arithmatex>\(x\)</span> 矩陣中的最大值</p> </div> <div class=tabbed-block> <p><span class=arithmatex>\(x\)</span> 矩陣的總項(欄)數</p> </div> </div> </div> <div class=i> <p>假設 <span class=arithmatex>\(L_m=\)</span> (你可以更改以下數字)</p> <!DOCTYPE html><html lang=zh-Hant> <head><meta charset=UTF-8><meta name=viewport content="width=device-width, initial-scale=1.0"><script src=https://cdn.jsdelivr.net/npm/chart.js></script><style>
        .input-container {
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
        }

        input[type="number"] {
            background-color: transparent;
            border: none;
            border-bottom: 2px solid #F2BBFF;
            font-size: 18px;
            padding: 5px;
            width: 60px;
            text-align: center;
            outline: none;
        }

        input[type="number"]:focus {
            border-bottom-color: #F2BBFF;
        }

        h1 {
            color: #333;
        }

        #chart-container {
            width: 80%;
            max-width: 600px;
            margin: 20px auto;
            background-color: white; /* 設置圖表容器背景為白色 */
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);
        }

        #output {
            font-size: 18px;
            margin-top: 20px;
        }
    </style></head> <body> <div class=input-container> <p>[</p> <input type=number id=num1 value=1> <p>,</p> <input type=number id=num2 value=2> <p>,</p> <input type=number id=num3 value=5> <p>,</p> <input type=number id=num4 value=3> <p>,</p> <input type=number id=num5 value=4> <p>]</p> </div> <div id=chart-container> <canvas id=probabilityChart></canvas> </div> <p id=output></p> <script>
        let chart; // 用於存儲圖表實例

        // Softmax 計算函數
        function calculateSoftmax() {
            const outputDiv = document.getElementById('output');
            outputDiv.innerHTML = ''; // 清空結果

            // 獲取輸入數值
            const inputs = [
                parseFloat(document.getElementById('num1').value),
                parseFloat(document.getElementById('num2').value),
                parseFloat(document.getElementById('num3').value),
                parseFloat(document.getElementById('num4').value),
                parseFloat(document.getElementById('num5').value)
            ];

            // 確保所有輸入是有效數字
            if (inputs.some(isNaN)) {
                outputDiv.textContent = '請確保所有輸入都是有效的數字。';
                return;
            }

            // 計算 Softmax 步驟
            let resultText = "";
            resultText += `原始數字：Lm=[x1,x2,...,xn]\n[${inputs.join(', ')}]\n\n`;

            // 最大值
            const maxVal = Math.max(...inputs);
            resultText += `最大值：m\n ${maxVal}\n\n`;

            // 減去最大值
            const subtracted = inputs.map(num => num - maxVal);
            resultText += `減去最大值：xi-m\n [${subtracted.join(', ')}]\n\n`;

            // 指數函數
            const expValues = subtracted.map(num => Math.exp(num));
            resultText += `計算指數函數：e^(xi):\n [${expValues.join(', ')}]\n\n`;

            // 總和
            const sumExp = expValues.reduce((a, b) => a + b, 0);
            resultText += `指數值總和：Σ(e^(xj):\n ${sumExp}\n\n`;

            // Softmax 值
            const softmaxValues = expValues.map(num => num / sumExp);
            resultText += `Softmax 值：e^(xi)/Σ(e^(xj)\n [${softmaxValues.join(', ')}]\n\n`;

            // 總和檢查
            const total = softmaxValues.reduce((a, b) => a + b, 0);
            resultText += `Softmax 總和 (應接近 1):\n ${total}\n\n`;

            outputDiv.innerHTML = resultText.replace(/\n/g, "<br>");

            // 繪製機率分布圖
            drawChart(softmaxValues);
        }

        // 繪製長條圖
        function drawChart(data) {
            const ctx = document.getElementById('probabilityChart').getContext('2d');

            // 如果圖表已經存在，先銷毀再重建
            if (chart) {
                chart.destroy();
            }

            chart = new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: ['x1', 'x2', 'x3', 'x4', 'x5'], // 標籤
                    datasets: [{
                        label: 'Softmax 機率分布',
                        data: data, // Softmax 值
                        backgroundColor: 'rgba(75, 192, 192, 0.6)',
                        borderColor: 'rgba(75, 192, 192, 1)',
                        borderWidth: 1
                    }]
                },
                options: {
                    responsive: true,
                    plugins: {
                        legend: {
                            display: true,
                            position: 'top'
                        }
                    },
                    scales: {
                        y: {
                            beginAtZero: true,
                            max: 1 // 確保機率分布在 [0, 1] 範圍內
                        }
                    }
                }
            });
        }

        // 監聽輸入框的變化
        document.querySelectorAll('.input-container input').forEach(input => {
            input.addEventListener('input', calculateSoftmax);
        });

        // 頁面加載時自動計算
        window.addEventListener('DOMContentLoaded', calculateSoftmax);
    </script> </body> </html> </div> <h2 id=損失函數>損失函數<a class=headerlink href=#損失函數 title="Permanent link"></a></h2> <p>我們要告訴人工智慧他找到的假設到底好不好，所以我們需要一個「評分標準」，被稱為損失函數。<br> 損失函數通常是計算「模型輸出」與「標籤(正確答案)」的差異程度，損失函數值越高，代表差異越大。</p> <p>回歸問題的損失函數：</p> <ul> <li><a href=#l1l2-loss>L1/L2 loss</a></li> </ul> <p>分類問題的損失函數：</p> <ul> <li><a href=#01-loss>0/1 loss</a></li> <li><a href=#negative-log-likelihoodnll>Negative Log-Likelihood(NLL)</a></li> </ul> <h3 id=回歸問題的損失函數>回歸問題的損失函數<a class=headerlink href=#回歸問題的損失函數 title="Permanent link"></a></h3> <h4 id=l1l2-loss>L1/L2 loss<a class=headerlink href=#l1l2-loss title="Permanent link"></a></h4> <p>輸出和標籤都是實數，所以計算他們之間的「距離」或是「距離平方」。</p> <pre class=arithmatex>\[
輸出：L_m =[ x_1 , x_2 , \cdots , x_n ]
\]</pre> <pre class=arithmatex>\[
標籤：y =[ y_1 , y_2 , \cdots , y_n ]
\]</pre> <pre class=arithmatex>\[
\text{L1 loss} = \sum_{i=1}^{n} \lvert x_i - y_i \rvert
\]</pre> <pre class=arithmatex>\[
\text{L2 loss} = \sum_{i=1}^{n} (x_i - y_i)^2
\]</pre> <p>舉例：</p> <pre class=arithmatex>\[
\begin{aligned}
L_m &amp;= [ 0 , 2 , 5 , 3 ] \\
y &amp;= [ 3 , 2 , 3 , 4 ] \\
\text{L1 loss} &amp;= \sum_{i=1}^{n} \lvert x_i - y_i \rvert  = 3+0+2+1 =6 \\
\text{L2 loss} &amp;= \sum_{i=1}^{n} (x_i - y_i)^2 = 9+0+4+1=14
\end{aligned}
\]</pre> <h3 id=分類問題的損失函數>分類問題的損失函數<a class=headerlink href=#分類問題的損失函數 title="Permanent link"></a></h3> <h4 id=01-loss>0/1 loss<a class=headerlink href=#01-loss title="Permanent link"></a></h4> <p>如果輸出和標籤一樣就得0分(沒有損失)，否則得1分(有損失)。</p> <pre class=arithmatex>\[
L(y, \hat{y}) =
\begin{cases} 
0 &amp; \text{if } \hat{y} = y \\
1 &amp; \text{if } \hat{y} \neq y
\end{cases}
\]</pre> <p>假設有一組真實標籤 <span class=arithmatex>\(y = [1, 0, 1]\)</span>，模型輸出 <span class=arithmatex>\(\hat{y} = [1, 1, 0]\)</span>，那麼 0/1 損失的值為：</p> <pre class=arithmatex>\[
L(y, \hat{y}) = [0, 1, 1]
\]</pre> <p>解釋：<br> <span class=arithmatex>\(y_1 = \hat{y_1}\)</span> 沒有損失，得到0分<br> <span class=arithmatex>\(y_2 \neq \hat{y_2}\)</span> 有損失，得到1分<br> <span class=arithmatex>\(y_3 \neq \hat{y_3}\)</span> 有損失，得到1分<br> 共2分</p> <h4 id=negative-log-likelihoodnll>Negative Log-Likelihood(NLL)<a class=headerlink href=#negative-log-likelihoodnll title="Permanent link"></a></h4> <p>根據標籤和輸出的機率計算損失函數。</p> <p>假設模型是輸入一張圖片，要判斷動物。<br> 標籤使用 one-hot vector 的形式，正確答案的動物種類為 1 ，其餘為 0 。</p> <pre class=arithmatex>\[
標籤：y =[ y_1 , y_2 , \cdots , y_n ]
\]</pre> <p>而輸出使用<a href=#softmax函數>softmax函數</a>。</p> <pre class=arithmatex>\[
輸出：L_m =[ x_1 , x_2 , \cdots , x_n ]
\]</pre> <p>NLL數學式：</p> <pre class=arithmatex>\[
\text{NLL} = - \sum_{i=1}^k y_i \log P(c_i | x)
\]</pre> <p>公式解釋：</p> <div class="tabbed-set tabbed-alternate" data-tabs=3:3><input checked=checked id=negative-log-likelihoodnll-c_i name=__tabbed_3 type=radio><input id=negative-log-likelihoodnll-pc_i--x name=__tabbed_3 type=radio><input id=negative-log-likelihoodnll-y_i name=__tabbed_3 type=radio><div class=tabbed-labels><label for=negative-log-likelihoodnll-c_i><span class=arithmatex>\(c_i\)</span></label><label for=negative-log-likelihoodnll-pc_i--x><span class=arithmatex>\(P(c_i | x)\)</span></label><label for=negative-log-likelihoodnll-y_i><span class=arithmatex>\(y_i\)</span></label></div> <div class=tabbed-content> <div class=tabbed-block> <p>所有可能的動物中的第 i 種動物。</p> </div> <div class=tabbed-block> <p>模型對圖片 <span class=arithmatex>\(x\)</span> 預測它屬於第 <span class=arithmatex>\(i\)</span> 種動物的概率。</p> </div> <div class=tabbed-block> <p>標籤（one-hot vector）的第 i 項，表示真實答案(標籤)是否為第 i 種，是的話就是 1 不是的話就是 0 。</p> </div> </div> </div> <p>計算範例(無條件捨去至小數第三位)：</p> <pre class=arithmatex>\[
標籤：y =[ 0 , 0 , 1 , 0 , 0 ] 
\]</pre> <pre class=arithmatex>\[
輸出：L_m =[0.0116, 0.0316, 0.6364, 0.0861, 0.2341] 
\]</pre> <pre class=arithmatex>\[
\begin{aligned}
NLL &amp;= -(0*\log 0.011 + 0*\log 0.031 + 1*\log 0.636 + 0*\log 0.086 + 0*\log  0.234) \\
&amp;= -\log 0.636  \\
&amp;= 0.197 
\end{aligned}
\]</pre> <p>我們試試看正確的類別是第二類別：</p> <pre class=arithmatex>\[
標籤：y =[ 0 , 1 , 0 , 0 , 0 ] 
\]</pre> <pre class=arithmatex>\[
輸出：L_m =[0.0116, 0.0316, 0.6364, 0.0861, 0.2341] 
\]</pre> <pre class=arithmatex>\[
\begin{aligned}
NLL &amp;= -(0*\log 0.011 + 1*\log 0.031 + 0*\log 0.636 + 0*\log 0.086 + 0*\log  0.234) \\
&amp;= -\log 0.031  \\
&amp;= 1.508 
\end{aligned}
\]</pre> <p>可以看到損失變大了</p> <h2 id=梯度下降演算法>梯度下降演算法<a class=headerlink href=#梯度下降演算法 title="Permanent link"></a></h2> <p>有了損失函數之後，我們訓練的目標就顯而易見了：「讓模型的 loss 越低越好」<br> 這時可以把機器學習看成一個最佳化問題：「模型的參數如何選擇時，可以得到最低的 loss，讓預測結果最接近真實答案」</p> <h3 id=一元函數梯度下降演算法>一元函數梯度下降演算法<a class=headerlink href=#一元函數梯度下降演算法 title="Permanent link"></a></h3> <p>為了解釋梯度下降演算法，我們先理解他的運作原理，我們用一個簡單的例子：</p> <blockquote> <p>不管輸入及輸出， loss function 為 <span class=arithmatex>\(loss=x^2\)</span> ，我們要如何找出最好的 x 讓 loss 最小。</p> </blockquote> <p>我們將 <span class=arithmatex>\(loss=x^2\)</span> 的圖形畫出來：</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-42.png></p> <p>顯而易見，我們要找出圖形中的極小值(最小的數值)，也就是最小的 <span class=arithmatex>\(loss(y)\)</span>，答案是：「<span class=arithmatex>\(x\)</span> 為 <span class=arithmatex>\(0\)</span> 時，有極小值 <span class=arithmatex>\(loss(y)\)</span> 為 <span class=arithmatex>\(0\)</span>」</p> <p>你可能會想說為何不直接微分 <span class=arithmatex>\([x^2]'=2x\)</span> ，得到 <span class=arithmatex>\(x\)</span> 為 <span class=arithmatex>\(0\)</span> 時有極小值。<br> 那是因為現在的函數為<a href=#一元函數-univariate-function>一元函數</a>，若為<a href=#多元函數-multivariate-function>多元函數</a>便無法做到。</p> <p>若使用梯度下降演算法，可以使用於夠複雜的<a href=#多元函數-multivariate-function>多元函數</a>，而我們現在先舉例<a href=#一元函數-univariate-function>一元函數</a>就好。</p> <h4 id=數學式>數學式<a class=headerlink href=#數學式 title="Permanent link"></a></h4> <p>梯度下降演算法數學式：</p> <pre class=arithmatex>\[
\theta_{i+1} := \theta_i - η \nabla_\theta f(\theta)
\]</pre> <p>解釋：</p> <div class="tabbed-set tabbed-alternate" data-tabs=4:4><input checked=checked id=數學式-theta_i1 name=__tabbed_4 type=radio><input id=數學式-theta_i name=__tabbed_4 type=radio><input id=數學式-η name=__tabbed_4 type=radio><input id=數學式-nabla_theta-ftheta name=__tabbed_4 type=radio><div class=tabbed-labels><label for=數學式-theta_i1><span class=arithmatex>\(\theta_{i+1}\)</span></label><label for=數學式-theta_i><span class=arithmatex>\(\theta_i\)</span></label><label for=數學式-η><span class=arithmatex>\(η\)</span></label><label for=數學式-nabla_theta-ftheta><span class=arithmatex>\(\nabla_\theta f(\theta)\)</span></label></div> <div class=tabbed-content> <div class=tabbed-block> <p>更新後的參數值，也就是在第 <span class=arithmatex>\(i+1\)</span> 次迭代後的參數。</p> </div> <div class=tabbed-block> <p>第 <span class=arithmatex>\(i\)</span> 次迭代的參數值。</p> </div> <div class=tabbed-block> <p>學習率 (Learning Rate)，用來控制每次更新步長的大小。</p> </div> <div class=tabbed-block> <p>損失函數 <span class=arithmatex>\(f(\theta)\)</span> 相對於參數 <span class=arithmatex>\(\theta\)</span> 的梯度。</p> </div> </div> </div> <p>我們針對現在的 loss function( <span class=arithmatex>\(loss=x^2\)</span> ) 寫一個更好懂的數學式：</p> <pre class=arithmatex>\[
x_{new} := x - η f'(x)
\]</pre> <p>解釋：</p> <div class="tabbed-set tabbed-alternate" data-tabs=5:4><input checked=checked id=數學式-x_new name=__tabbed_5 type=radio><input id=數學式-x name=__tabbed_5 type=radio><input id=數學式-η_1 name=__tabbed_5 type=radio><input id=數學式-fx name=__tabbed_5 type=radio><div class=tabbed-labels><label for=數學式-x_new><span class=arithmatex>\(x_{new}\)</span></label><label for=數學式-x><span class=arithmatex>\(x\)</span></label><label for=數學式-η_1><span class=arithmatex>\(η\)</span></label><label for=數學式-fx><span class=arithmatex>\(f'(x)\)</span></label></div> <div class=tabbed-content> <div class=tabbed-block> <p>更新之後的x數值。</p> </div> <div class=tabbed-block> <p>原本的x數值。</p> </div> <div class=tabbed-block> <p>這是學習率 (Learning Rate)，用來控制每次更新步長的大小。</p> </div> <div class=tabbed-block> <p><span class=arithmatex>\(x\)</span> 在 <span class=arithmatex>\(f(x)\)</span> 時的斜率。</p> </div> </div> </div> <p>接下來我們提出幾個問題</p> <ol> <li> <p>為何算式中有 f'(x)<br> <span class=arithmatex>\(f'(x)\)</span>代表斜率。<br> 斜率為正時，代表在 <span class=arithmatex>\(x\)</span> 附近遞增，便意味著若要讓 <span class=arithmatex>\(f(x)\)</span> 減少(下降)，<span class=arithmatex>\(x\)</span> 就要往負的地方移動，也就是把 <span class=arithmatex>\(x\)</span> 減去 <span class=arithmatex>\(f'(x)\)</span> 。<br> 若斜率為負則反之。</p> </li> <li> <p>為何要使用學習率<br> 假設原本 <span class=arithmatex>\(x\)</span> 是 <span class=arithmatex>\(10\)</span>，<span class=arithmatex>\(f(x)=x^2\)</span>，<span class=arithmatex>\(f'(x)=2x\)</span>。<br> 第一次更新： <span class=arithmatex>\(-10 = 10-2*10\)</span><br> 第二次更新： <span class=arithmatex>\(10 = -10 - 2*(-10)\)</span><br> 第三次更新： <span class=arithmatex>\(-10 = 10-2*10\)</span><br> 第四次更新： <span class=arithmatex>\(10 = -10 - 2*(-10)\)</span><br> 可以發現，這樣會一直循環沒有結果，我們可以試著在更新越多次時，每次走的更少，所增加了學習率：<br> 第一次更新： <span class=arithmatex>\(-6 = 10 - 0.8*2*10\)</span><br> 第二次更新： <span class=arithmatex>\(4 = -6 - 0.8*2*(-6)\)</span><br> 第三次更新： <span class=arithmatex>\(-2 = 4 - 0.8*2*4\)</span><br> 這樣就不會鬼牆了</p> </li> </ol> <p>接下來我們操作一次，我們除了上述變數，還要設定「收斂條件」及「最大跌代次數」，不然程式會沒有終止條件，可能永遠不會結束。</p> <ul> <li>收斂條件：模型可能永遠不會達到斜率為0的最低點，要走到的機率也很小，所以我們要設定一個值，也就是符合我們期待的斜率，跟模型說他訓練的夠好了。</li> <li>最大跌代次數：防止無限迭代，及避免過度計算，節省運算資源。</li> </ul> <h4 id=模擬>模擬<a class=headerlink href=#模擬 title="Permanent link"></a></h4> <div class=i> <p>你可以更改以下數字(初始值不可更改)，你可以從「最大迭代次數 (max iterations)=0」開始，慢慢增加迭代次數，看看迭代過程。</p> <hr> <!DOCTYPE html><html lang=zh-Hant> <head><meta charset=UTF-8><meta name=viewport content="width=device-width, initial-scale=1.0"><script src=https://cdn.jsdelivr.net/npm/chart.js></script><style>
        .input-container {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin-bottom: 20px;
        }
        input[type="number"] {
            background-color: transparent;
            border: none;
            border-bottom: 2px solid #F2BBFF;
            font-size: 16px;
            padding: 5px;
            width: 100px;
            text-align: center;
            outline: none;
        }
        input[type="number"]:focus {
            border-bottom-color: #F2BBFF;
        }
        h1, h2 {
            color: #333;
        }
        .chart-container {
            width: 90%;
            max-width: 800px;
            margin: 20px auto;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);
            background-color: white; /* 設置背景為白色 */
        }
        #output {
            font-size: 16px;
            white-space: pre-line; /* 保持換行格式 */
        }
    </style></head> <body> <div class=input-container> <div> <label>初始值 (x): 10</label> </div> <div> <label>學習率 (learning rate): <input type=number id=learningRate value=0.7 step=0.1></label> </div> <div> <label>最大迭代次數 (max iterations): <input type=number id=maxIterations value=50 step=1></label> </div> <div> <label>收斂條件 (tolerance): <input type=number id=tolerance value=0.1 step=0.01></label> </div> </div> <div class=chart-container> <canvas id=gradientChart></canvas> </div> <div class=chart-container> <canvas id=lossChart></canvas> </div> <div class=chart-container> <canvas id=xChart></canvas> </div> <p id=gradientOutput></p> <script>
        let lossChart, xChart, gradientChart;

        // 定義損失函數和梯度
        function lossFunction(x) {
            return x ** 2;
        }

        function gradient(x) {
            return 2 * x;
        }

        // 計算梯度下降並更新圖表
        function runGradientDescent() {
            const outputDiv = document.getElementById('gradientOutput');
            outputDiv.innerHTML = ''; // 清空結果

            // 獲取參數
            const x0 = 10;
            const learningRate = parseFloat(document.getElementById('learningRate').value);
            const maxIterations = parseInt(document.getElementById('maxIterations').value);
            const tolerance = parseFloat(document.getElementById('tolerance').value);

            let x = x0;
            let iteration = 0;
            const xValues = [x];
            const lossValues = [lossFunction(x)];
            const iterationIndices = [0];

            let resultText = `開始梯度下降:\n初始值 x = ${x0}, 學習率 = ${learningRate}, 最大迭代次數 = ${maxIterations}, 收斂條件 = ${tolerance}\n\n`;

            resultText += `計算: x_new = x - η * f'(x) ：\n`;

            while (iteration < maxIterations) {
                const grad = gradient(x);
                const loss = lossFunction(x);
                const prex = x;

                // 更新 x
                x = x - learningRate * grad;

                resultText += `${x.toFixed(6)} = ${prex.toFixed(6)} - ${learningRate} * ${grad.toFixed(6)}\n`;

                iteration++;
                xValues.push(x);
                lossValues.push(lossFunction(x));
                iterationIndices.push(iteration);

                if (Math.abs(grad) < tolerance) {
                    resultText += `梯度下降已收斂 ( ${Math.abs(grad).toFixed(6)}<${tolerance} ) ，停止迭代。\n\n`;
                    break;
                }
            }

            resultText += `最終結果: x = ${x.toFixed(6)}, Loss = ${lossFunction(x).toFixed(6)}, 迭代次數 = ${iteration}\n`;

            // 使用 innerHTML 來顯示換行
            outputDiv.innerHTML = resultText.replace(/\n/g, "<br>");

            // 更新圖表
            drawGradientChart(xValues, lossValues);
            drawLossChart(iterationIndices, lossValues);
            drawXChart(iterationIndices, xValues);
        }

        // 繪製梯度下降圖表
        function drawGradientChart(xValues, lossValues) {
            const ctx = document.getElementById('gradientChart').getContext('2d');
            const xRange = Array.from({ length: 500 }, (_, i) => -12 + (i * 24) / 500);
            const yRange = xRange.map(x => lossFunction(x));
            if (gradientChart) gradientChart.destroy();
            gradientChart = new Chart(ctx, {
                type: 'line',
                data: {
                    labels: xRange,
                    datasets: [
                        {
                            label: 'Gradient Descent Steps',
                            data: xValues.map((x, i) => ({ x, y: lossValues[i] })),
                            type: 'scatter',
                            backgroundColor: 'red',
                            pointRadius: 5
                        },
                        {
                            label: 'y = x^2',
                            data: yRange,
                            borderColor: 'blue',
                            fill: false,
                            pointRadius: 1
                        }
                    ]
                },
                options: {
                    responsive: true,
                    scales: {
                        x: { type: 'linear', title: { display: true, text: 'x Value' } },
                        y: { title: { display: true, text: 'y (Loss)' } }
                    }
                }
            });
        }

        // 繪製損失值變化圖表
        function drawLossChart(iterationIndices, lossValues) {
            const ctx = document.getElementById('lossChart').getContext('2d');
            if (lossChart) lossChart.destroy();
            lossChart = new Chart(ctx, {
                type: 'line',
                data: {
                    labels: iterationIndices,
                    datasets: [{
                        label: 'Loss Value Over Iterations',
                        data: lossValues,
                        borderColor: 'blue',
                        fill: false
                    }]
                },
                options: {
                    responsive: true,
                    plugins: {
                        legend: { display: true }
                    },
                    scales: {
                        x: { title: { display: true, text: 'Iterations' } },
                        y: { title: { display: true, text: 'Loss Value' } }
                    }
                }
            });
        }

        // 繪製 x 值變化圖表
        function drawXChart(iterationIndices, xValues) {
            const ctx = document.getElementById('xChart').getContext('2d');
            if (xChart) xChart.destroy();
            xChart = new Chart(ctx, {
                type: 'line',
                data: {
                    labels: iterationIndices,
                    datasets: [{
                        label: 'x Value Over Iterations',
                        data: xValues,
                        borderColor: 'orange',
                        fill: false
                    }]
                },
                options: {
                    responsive: true,
                    plugins: {
                        legend: { display: true }
                    },
                    scales: {
                        x: { title: { display: true, text: 'Iterations' } },
                        y: { title: { display: true, text: 'x Value' } }
                    }
                }
            });
        }

        // 監聽輸入框變化並重新計算
        document.querySelectorAll('.input-container input').forEach(input => {
            input.addEventListener('input', runGradientDescent);
        });

        // 初次加載時運行梯度下降
        window.addEventListener('DOMContentLoaded', runGradientDescent);
    </script> </body> </html> <p>(內容消失時請重新整理或按下f5)</p> </div> <p>這個例子和實際的梯度下降演算法還是有一些差異喔！！</p> <h4 id=備註>備註<a class=headerlink href=#備註 title="Permanent link"></a></h4> <h5 id=一元函數-univariate-function>一元函數 (Univariate Function)<a class=headerlink href=#一元函數-univariate-function title="Permanent link"></a></h5> <p>只有一個自變量 <span class=arithmatex>\( x \)</span>，形式為 <span class=arithmatex>\( f(x) \)</span>，輸入一個變量，輸出一個值。</p> <p>例子：</p> <ul> <li><span class=arithmatex>\( f(x) = x^2 \)</span></li> <li><span class=arithmatex>\( f(x) = 3x + 5 \)</span></li> </ul> <h5 id=多元函數-multivariate-function>多元函數 (Multivariate Function)<a class=headerlink href=#多元函數-multivariate-function title="Permanent link"></a></h5> <p>有多個(兩個以上)自變量，形式為 <span class=arithmatex>\( f(x_1, x_2, \dots, x_n) \)</span>，輸入多個變量，輸出一個值。</p> <p>例子：</p> <ul> <li><span class=arithmatex>\( f(x, y) = x^2 + y^2 \)</span></li> <li><span class=arithmatex>\( f(x, y, z) = x + y + z \)</span></li> </ul> <h3 id=多元函數梯度下降演算法>多元函數梯度下降演算法<a class=headerlink href=#多元函數梯度下降演算法 title="Permanent link"></a></h3> <p>有了損失函數之後，我們訓練的目標就顯而易見了：「讓模型的 loss 越低越好」<br> 這時可以把機器學習看成一個最佳化問題：「模型的參數為多少時，可以得到最低的 loss」</p> <p>我們假設損失函數為 <a href=#l1l2-loss>L2 loss</a> ，看看如何執行梯度下降演算法，模型如下圖。</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/image-41.png></p> <p>這個模型輸入數值 <span class=arithmatex>\(x\)</span> ， 計算之後輸出 <span class=arithmatex>\(\hat{y}\)</span> ，再其中我們希望<a href=%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%28Machine%20Learning%29.html#學習演算法>學習演算法</a>調整的參數有 <span class=arithmatex>\(w\)</span> 及 <span class=arithmatex>\(b\)</span> ，而他使用 <a href=#l1l2-loss>L2 loss</a> 作為 loss function，如下。</p> <pre class=arithmatex>\[
L2 \_ loss = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = f(x)= \sum_{i=1}^{n} (y_i - (wx_i + b))^2 
\]</pre> <p>所以我們的目的便是：「找到最一個 <span class=arithmatex>\((w,x)\)</span> 組合，使 <span class=arithmatex>\(L2 \_ loss\)</span> 的數值最小」。</p> <p>我們回憶一下一元函數梯度下降的數學式：</p> <pre class=arithmatex>\[
\theta_{i+1} := \theta_i - η \nabla_\theta f(\theta)
\]</pre> <p>而我們要找的變數有 <span class=arithmatex>\(w\)</span> 、 <span class=arithmatex>\(b\)</span> ，可以得到兩個數學式：</p> <pre class=arithmatex>\[
w_{new} := w - η \frac{\partial L2\_loss}{\partial w}
\]</pre> <pre class=arithmatex>\[
b_{new} := b - η \frac{\partial L2\_loss}{\partial b}
\]</pre> <p>解釋：</p> <div class="tabbed-set tabbed-alternate" data-tabs=6:4><input checked=checked id=多元函數梯度下降演算法-w_new name=__tabbed_6 type=radio><input id=多元函數梯度下降演算法-w name=__tabbed_6 type=radio><input id=多元函數梯度下降演算法-η name=__tabbed_6 type=radio><input id=多元函數梯度下降演算法-fracpartial-l2_losspartial-w name=__tabbed_6 type=radio><div class=tabbed-labels><label for=多元函數梯度下降演算法-w_new><span class=arithmatex>\(w_{new}\)</span></label><label for=多元函數梯度下降演算法-w><span class=arithmatex>\(w\)</span></label><label for=多元函數梯度下降演算法-η><span class=arithmatex>\(η\)</span></label><label for=多元函數梯度下降演算法-fracpartial-l2_losspartial-w><span class=arithmatex>\(\frac{\partial L2\_loss}{\partial w}\)</span></label></div> <div class=tabbed-content> <div class=tabbed-block> <p>更新之後的w數值。</p> </div> <div class=tabbed-block> <p>原本的w數值。</p> </div> <div class=tabbed-block> <p>這是學習率 (Learning Rate)，用來控制每次更新步長的大小。</p> </div> <div class=tabbed-block> <p>是損失函數(L2 loss)對偏差 <span class=arithmatex>\(w\)</span> 的<a href=#偏微分>偏微分</a></p> </div> </div> </div> <div class="tabbed-set tabbed-alternate" data-tabs=7:4><input checked=checked id=多元函數梯度下降演算法-b_new name=__tabbed_7 type=radio><input id=多元函數梯度下降演算法-b name=__tabbed_7 type=radio><input id=多元函數梯度下降演算法-η_1 name=__tabbed_7 type=radio><input id=多元函數梯度下降演算法-fracpartial-l2_losspartial-b name=__tabbed_7 type=radio><div class=tabbed-labels><label for=多元函數梯度下降演算法-b_new><span class=arithmatex>\(b_{new}\)</span></label><label for=多元函數梯度下降演算法-b><span class=arithmatex>\(b\)</span></label><label for=多元函數梯度下降演算法-η_1><span class=arithmatex>\(η\)</span></label><label for=多元函數梯度下降演算法-fracpartial-l2_losspartial-b><span class=arithmatex>\(\frac{\partial L2\_loss}{\partial b}\)</span></label></div> <div class=tabbed-content> <div class=tabbed-block> <p>更新之後的w數值。</p> </div> <div class=tabbed-block> <p>原本的b數值。</p> </div> <div class=tabbed-block> <p>這是學習率 (Learning Rate)，用來控制每次更新步長的大小。</p> </div> <div class=tabbed-block> <p>是損失函數(L2 loss)對偏差 <span class=arithmatex>\(b\)</span> 的<a href=#偏微分>偏微分</a></p> </div> </div> </div> <p>我們先將所有會用到的都計算好<br> 模型為： </p> <pre class=arithmatex>\[
\hat{y}=wx+b
\]</pre> <p>損失函數：</p> <pre class=arithmatex>\[
loss = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = f(x)= \sum_{i=1}^{n} (y_i - (wx_i + b))^2 
\]</pre> <p>loss對w偏導：<br> 我們先不管<span class=arithmatex>\(\sum\)</span>，最後再加入(根據<a href=%CE%A3%20sigma%20%28%E8%A5%BF%E6%A0%BC%E7%91%AA%29.html#-的性質><span class=arithmatex>\(\sum\)</span>的性質</a>)</p> <pre class=arithmatex>\[
\begin{aligned}
令loss &amp;= \sum_{i=1}^{n} loss',\ loss' =  (y_i - (wx + b))^2  \\
\frac{\partial loss'}{\partial w} &amp;= 2(y_i - (wx + b))^{2-1} \cdot  \frac{\partial }{\partial w}(y_i - (wx + b)) \\
&amp;=2(y_i - (wx + b)) \cdot \frac{\partial }{\partial w}(wx) \\
&amp;=2(y_i - (wx + b)) \cdot x  \\
&amp;=2x(y_i - (wx + b)) \\
故\frac{\partial loss}{\partial w} &amp;= \sum_{i=1}^{n} 2x(y_i - (wx + b))= \sum_{i=1}^{n} 2x(y_i - \hat{y})
\end{aligned}
\]</pre> <p>loss對b偏導：<br> 我們先不管<span class=arithmatex>\(\sum\)</span>，最後再加入(根據<a href=%CE%A3%20sigma%20%28%E8%A5%BF%E6%A0%BC%E7%91%AA%29.html#-的性質><span class=arithmatex>\(\sum\)</span>的性質</a>)</p> <pre class=arithmatex>\[
\begin{aligned}
令loss &amp;= \sum_{i=1}^{n} loss',\ loss' =  (y_i - (wx + b))^2  \\
\frac{\partial loss'}{\partial b} &amp;= 2(y_i - (wx + b))^{2-1} \cdot  \frac{\partial }{\partial b}(y_i - (wx + b)) \\
&amp;=2(y_i - (wx + b)) \cdot \frac{\partial }{\partial b}(b) \\
&amp;=2(y_i - (wx + b)) \cdot 1  \\
&amp;=2(y_i - (wx + b)) \\
故\frac{\partial loss}{\partial b} &amp;= \sum_{i=1}^{n} 2(y_i - (wx + b))= \sum_{i=1}^{n} 2(y_i - \hat{y})
\end{aligned}
\]</pre> <h4 id=計算流程>計算流程<a class=headerlink href=#計算流程 title="Permanent link"></a></h4> <div class=i> <p>// 宣告 w 、 b<br> w = 5 <br> b = 5</p> <p>重複「最大跌帶次數」次：</p> <p>&nbsp; // 從訓練資料中拿取一個<a href=#batch>Batch</a>的資料<br> &nbsp; X = [1, 2, 3, 4] <br> &nbsp; // 拿取的訓練資料(X)的正確答案(標籤)<br> &nbsp; y = [5, 7, 9, 11]<br> &nbsp; <br> &nbsp; // 宣告一個預測值陣列，<span class=arithmatex>\(\hat{y}=[wx_i+b,wx_{i+1}+b,...,wx_i+b]\)</span><br> &nbsp; y_hat = w * X(陣列) + b <br> &nbsp; <br> &nbsp; // 計算損失 (L2 Loss)，<span class=arithmatex>\(loss=\sum_{i=1}^{n} (y_i - \hat{y}_i)^2\)</span> <br> &nbsp; loss = sigma((y - y_hat) ** 2) <br> &nbsp; <br> &nbsp; // 計算w梯度<span class=arithmatex>\(\frac{\partial loss}{\partial w} = \sum_{i=1}^{n} 2x(y_i - (wx + b))= \sum_{i=1}^{n} 2x(y_i - \hat{y})\)</span><br> &nbsp; grad_w = -2 * sigma(X * (y - y_hat))<br> &nbsp; // 計算b梯度<span class=arithmatex>\(\frac{\partial loss}{\partial b} = \sum_{i=1}^{n} 2(y_i - (wx + b))= \sum_{i=1}^{n} 2(y_i - \hat{y})\)</span><br> &nbsp; grad_b = -2 * sigma(y - y_hat) <br> &nbsp; <br> &nbsp; //更新w參數<br> &nbsp; w = w - eta * grad_w <br> &nbsp; //更新b參數<br> &nbsp; b = b - eta * grad_b <br> &nbsp; <br> &nbsp; // 收斂條件判斷：如果梯度的大小小於收斂閾值，停止迭代<br> &nbsp; if sqrt(grad_w ** 2 + grad_b ** 2) &lt; 收斂閾值:<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;離開迴圈</p> <p>最後得到 w 、 b</p> </div> <h4 id=模擬_1>模擬<a class=headerlink href=#模擬_1 title="Permanent link"></a></h4> <div class=i> <p>輸入 X = [1, 2, 3, 4]<br> 標籤 y = [5, 7, 9, 11]<br> 模型 y = wx + b<br> 初始 w = 5<br> 初始 b = 5</p> <!DOCTYPE html><html lang=zh-Hant> <head><meta charset=UTF-8><meta name=viewport content="width=device-width, initial-scale=1.0"><script src=https://cdn.jsdelivr.net/npm/plotly.js-dist@2.16.1></script><title>梯度下降3D示意圖 (模型2)</title><style>
        .input-container {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin-bottom: 20px;
        }
        input[type="number"] {
            background-color: transparent;
            border: none;
            border-bottom: 2px solid #F2BBFF;
            font-size: 16px;
            padding: 5px;
            width: 100px;
            text-align: center;
            outline: none;
        }
        input[type="number"]:focus {
            border-bottom-color: #F2BBFF;
        }
        h1, h2 {
            color: #333;
        }
        .chart-container {
            width: 90%;
            max-width: 800px;
            margin: 20px auto;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);
            background-color: white;
        }
        #model2_output {
            font-size: 16px;
            white-space: pre-line;
        }
    </style></head> <body> <div class=input-container> <label>學習率 (learning rate): <input type=number id=model2_learningRate value=0.01 step=0.01></label> <label>最大迭代次數 (max iterations): <input type=number id=model2_maxIterations value=300 step=1></label> <label>收斂條件 (tolerance): <input type=number id=model2_tolerance value=0.1 step=0.01></label> </div> <div class=chart-container> <div id=model2_3d_chart></div> </div> <p id=model2_output></p> <script>
        function model2_runGradientDescent() {
            const outputDiv = document.getElementById('model2_output');
            outputDiv.innerHTML = ''; // 清空結果

            // 獲取用戶輸入的參數
            const learningRate = parseFloat(document.getElementById('model2_learningRate').value);
            const maxIterations = parseInt(document.getElementById('model2_maxIterations').value);
            const tolerance = parseFloat(document.getElementById('model2_tolerance').value);

            // 訓練數據
            const X = [1, 2, 3, 4];
            const y = [5, 7, 9, 11];

            let model2_w = 5, model2_b = 5;  // 初始值
            let model2_wVals = [model2_w];
            let model2_bVals = [model2_b];
            let model2_lossVals = [];
            let model2_iterationIndices = [];
            let model2_iteration = 0;

            // 用來存儲每次迭代的損失
            let resultText = `開始梯度下降:\n初始值 w = ${model2_w}, b = ${model2_b}, 學習率 = ${learningRate}, 最大迭代次數 = ${maxIterations}, 收斂條件 = ${tolerance}\n\n`;

            resultText += `計算: w_new = w - η * f'(w) ：\n`;

            while (model2_iteration < maxIterations) {
                const y_hat = X.map(xi => model2_w * xi + model2_b);
                const loss = y.reduce((sum, yi, i) => sum + Math.pow(y_hat[i] - yi, 2), 0);
                const grad_w = -2 * X.reduce((sum, xi, i) => sum + xi * (y[i] - y_hat[i]), 0);
                const grad_b = -2 * y.reduce((sum, yi, i) => sum + (yi - y_hat[i]), 0);

                model2_w -= learningRate * grad_w;
                model2_b -= learningRate * grad_b;

                model2_wVals.push(model2_w);
                model2_bVals.push(model2_b);
                model2_lossVals.push(loss);
                model2_iterationIndices.push(model2_iteration);

                if (Math.sqrt(grad_w**2 + grad_b**2) < tolerance) {
                    resultText += `梯度下降已收斂 ( 梯度 = ${Math.sqrt(grad_w**2 + grad_b**2).toFixed(6)} < ${tolerance} )，停止迭代。\n\n`;
                    break;
                }

                model2_iteration++;
            }

            resultText += `最終結果: w = ${model2_w.toFixed(6)}, b = ${model2_b.toFixed(6)}, 迭代次數 = ${model2_iteration}\n`;

            outputDiv.innerHTML = resultText.replace(/\n/g, "<br>");

            // 繪製 3D 圖表
            model2_draw3DChart(model2_wVals, model2_bVals, model2_lossVals);
        }

        // 使用 Plotly 繪製 3D 圖表
        function model2_draw3DChart(wVals, bVals, lossVals) {
            const trace1 = {
                x: wVals,
                y: bVals,
                z: lossVals,
                mode: 'markers',
                type: 'scatter3d',
                marker: { color: 'red', size: 5 }
            };
            const data = [trace1];

            const layout = {
                scene: {
                    xaxis: { title: 'Weight (w)' },
                    yaxis: { title: 'Bias (b)' },
                    zaxis: { title: 'Loss Value (loss)' },
                    camera: {
                        eye: {
                            x: -2, // 攝影機的 x 軸位置
                            y: -2, // 攝影機的 y 軸位置
                            z: 1.5  // 攝影機的 z 軸位置
                        }
                    }
                },
                title: ''
            };

            Plotly.newPlot('model2_3d_chart', data, layout);
        }


        // 監聽輸入框變化並重新計算
        document.querySelectorAll('.input-container input').forEach(input => {
            input.addEventListener('input', model2_runGradientDescent);
        });

        // 初次加載時運行梯度下降
        window.addEventListener('DOMContentLoaded', model2_runGradientDescent);
    </script> </body> </html> <p>(內容消失時請重新整理或按下f5)</p> </div> <h4 id=補充>補充<a class=headerlink href=#補充 title="Permanent link"></a></h4> <p>如果覺得對於梯度下降演算法不夠熟悉，可以參考以下兩個影片<br> <a href="https://www.youtube.com/watch?v=UkcUZTe49Pg">https://www.youtube.com/watch?v=UkcUZTe49Pg</a> <br> <a href="https://www.youtube.com/watch?v=s7BxboxEfnU">https://www.youtube.com/watch?v=s7BxboxEfnU</a> </p> <h4 id=備註_1>備註<a class=headerlink href=#備註_1 title="Permanent link"></a></h4> <h5 id=偏微分>偏微分<a class=headerlink href=#偏微分 title="Permanent link"></a></h5> <p>我們先來講一元函數微分，公式 ：</p> <pre class=arithmatex>\[
\frac{d}{dx}(x^n) = n \cdot x^{n-1}
\]</pre> <p>所以</p> <pre class=arithmatex>\[
f(x) = x^2 + x + 1
\]</pre> <pre class=arithmatex>\[
f'(x) = \frac{df}{dx} = 2x^{2-1} + x^{1-1} + 0 \cdot 1 \cdot x^{0-1} = 2x + 1 + 0 = 2x + 1
\]</pre> <hr> <p>接下來我們看看偏微分</p> <p>偏微分是針對多變數函數的一種微分方法。在偏微分中，我們選擇其中的一個變數進行微分，其他變數視為常數。</p> <p>對於一個多變數函數 <span class=arithmatex>\(f(x, y, z)\)</span>，如果只對變數 <span class=arithmatex>\(z\)</span> 求導，記作：</p> <pre class=arithmatex>\[
\frac{\partial f}{\partial x}
\]</pre> <p>這表示只考慮 <span class=arithmatex>\(z\)</span> 的變化，而把 <span class=arithmatex>\(x\)</span> 和 <span class=arithmatex>\(y\)</span> 當作常數。</p> <p>假設有一個函數：</p> <pre class=arithmatex>\[
f(x, y, z) = x^2 + x + y^3 + y^2 + z^3 + z^2 + z + 1
\]</pre> <p>對於這個函數，只有包含 <span class=arithmatex>\(z\)</span> 的項會影響偏導數，與 <span class=arithmatex>\(z\)</span> 無關的部分視為常數，其導數為 <span class=arithmatex>\(0\)</span>。<br> 包含 <span class=arithmatex>\(z\)</span> 的項為：</p> <pre class=arithmatex>\[
z^3 + z^2 + z
\]</pre> <p>其餘項 <span class=arithmatex>\(x^2 + x + y^3 + y^2 + 1\)</span> 的偏導數為 <span class=arithmatex>\(0\)</span>，因為它們與 <span class=arithmatex>\(z\)</span> 無關。</p> <p>求偏導數：</p> <p>逐項計算對 <span class=arithmatex>\(z\)</span> 的偏導數：</p> <ol> <li><span class=arithmatex>\(\frac{\partial}{\partial z}(z^3) = 3z^2\)</span> （乘冪微分公式 <span class=arithmatex>\(\frac{d}{dx}(x^n) = n \cdot x^{n-1}\)</span>）</li> <li><span class=arithmatex>\(\frac{\partial}{\partial z}(z^2) = 2z\)</span></li> <li><span class=arithmatex>\(\frac{\partial}{\partial z}(z) = 1\)</span></li> </ol> <hr> <p>綜合結果：</p> <p>將偏導數加總：</p> <pre class=arithmatex>\[
\frac{\partial f}{\partial z} = 3z^2 + 2z + 1
\]</pre> <hr> <p>最終答案：</p> <p>對 <span class=arithmatex>\(z\)</span> 偏微分的結果為：</p> <pre class=arithmatex>\[
\frac{\partial f}{\partial z} = 3z^2 + 2z + 1
\]</pre> <h5 id=batch>Batch<a class=headerlink href=#batch title="Permanent link"></a></h5> <p>在機器學習中，<strong>batch</strong>（批次）指的是在訓練過程中，每次用來更新模型參數的一組數據樣本。訓練過程中的數據通常不是一次性全部傳遞給模型，而是分批進行處理，每一批數據稱為「mini-batch」。</p> <p>為什麼要使用 batch？<br> 1. <strong>計算效率</strong>：將數據分批處理有助於減少計算量，並且可以充分利用硬體資源（如 GPU），這樣可以提高訓練效率。</p> <ol> <li> <p><strong>內存限制</strong>：如果數據集非常大，一次性將所有數據加載到內存中可能不現實，分批處理可以減少內存的需求。</p> </li> <li> <p><strong>梯度估計</strong>：每個批次中的數據樣本不足以計算整批次的梯度。這也使用迷你批量梯度集來計算梯度（全批次梯度下降）更快，但依然能夠達到足夠長的性能。因此比單一樣本更新（隨機梯度下降）來得穩定。</p> </li> </ol> <h2 id=幫助訓練神經網路>幫助訓練神經網路<a class=headerlink href=#幫助訓練神經網路 title="Permanent link"></a></h2> <h3 id=dropout>Dropout<a class=headerlink href=#dropout title="Permanent link"></a></h3> <p><a href=https://datasciocean.tech/deep-learning-core-concept/understand-dropout-in-deep-learning/ >參考資料</a></p> <p>這是一種<a href=%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%28Machine%20Learning%29.html#regularization正規化>Generalization</a>的方式，能夠有效防止<a href=%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%28Machine%20Learning%29.html#overfitting過擬合>overfitting</a>。<br> 如下圖，(a)是一般的神經網路，(b)是加上 Dropout 機制的神經元。<br> Dropout 就是在訓練時將幾個神經元隨機關閉進行訓練，避免每個神經元「太專一」，也就是避免每個神經元只能做同樣的事情。</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/dropout.png><br> <a href=https://datasciocean.tech/deep-learning-core-concept/understand-dropout-in-deep-learning/ >圖片來源</a></p> <p>如下圖，Dropout 與一般神經網路的差異就是於神經元計算完成後乘上 r ，這個 r 可能是 0 或是 1 ，也就是決定是否啟用神經元(決定是否採用神經元的輸出)。</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/dropout-network-vs-standard-network.png><br> <a href=https://datasciocean.tech/deep-learning-core-concept/understand-dropout-in-deep-learning/ >圖片來源</a></p> <p>數學式如下，紅色是 Dropout 機制才有的計算，可以看到透過Bernoulli(<a href=https://zh.wikipedia.org/zh-tw/%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%88%86%E5%B8%83>伯努利分布</a>，也就是隨機的方式)得到 r 為 1 或是 0 ，再將神經元輸出乘上 r ，用來決定是否要將神經元的輸出加入計算。</p> <p><img alt="alt text" src=images/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28Neural%20Network%29/dropout-formula.png><br> <a href=https://datasciocean.tech/deep-learning-core-concept/understand-dropout-in-deep-learning/ >圖片來源</a></p> <h3 id=batch-normalization>Batch Normalization<a class=headerlink href=#batch-normalization title="Permanent link"></a></h3> <p>Batch Normalization 的作法就是對每一個 mini-batch 都進行正規化，將數據轉換為均值為0、標準差為1的常態分佈。這樣可以將分散的數據統一，有助於減緩梯度消失問題，並解決內部協方差偏移（Internal Covariate Shift）的問題。由於每層的輸入數據保持穩定，模型能夠更快速地收斂。此外，Batch Normalization 還具有<a href=%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%28Machine%20Learning%29.html#generalization泛化>generalization</a>的效果，這使得模型在某些情況下可以不使用 <a href=#dropout>Dropout</a> 等其他<a href=%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%28Machine%20Learning%29.html#generalization泛化>generalization</a>技術。總結來說，Batch Normalization 可以提高訓練效率、穩定性，並減少過擬合的風險。</p> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=footer.title> <a href=%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%28Machine%20Learning%29.html class="md-footer__link md-footer__link--prev" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Previous </span> 機器學習(Machine Learning) </div> </div> </a> <a href=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%BC%94%E7%AE%97%E6%B3%95.html class="md-footer__link md-footer__link--next" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Next </span> 梯度下降演算法 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> with emoji by <a href=https://github.com/twitter/twemoji target=_blank rel=noopener> Twemoji </a> </div> <div class=md-social> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": ".", "features": ["announce.dismiss", "sitemap", "navigation.tabs", "navigation.top", "navigation.instant", "navigation.indexes", "toc.follow", "content.tabs.link", "search.share", "search.suggest", "content.code.copy", "content.code.annotations"], "search": "assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=assets/javascripts/bundle.471ce7a9.min.js></script> <script src=https://unpkg.com/mermaid@10.6.1/dist/mermaid.min.js></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=assets/pymdownx-extras/extra-loader-iLQ-keay.js></script> </body> </html>